{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [AWS Tutorial: Youtube Tutorials on AWS Glue](https://www.youtube.com/playlist?list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Documentation: [awsglue](https://github.com/awslabs/aws-glue-libs/tree/master/awsglue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [AWS Tutorials - Data Quality Check in AWS Glue ETL Pipeline](https://www.youtube.com/watch?v=44PbyHE57aM&t=111s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data_quality_check](./data_quality_check.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reusable Workflow Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"Comment\": \"Data Quality Check Workflow\",\n",
    "  \"StartAt\": \"StartProfileJob\",\n",
    "  \"States\": {\n",
    "    \"StartProfileJob\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::databrew:startJobRun.sync\",\n",
    "      \"Parameters\": {\n",
    "        \"Name.$\": \"$.profilejobname\"\n",
    "      },\n",
    "      \"Next\": \"CheckDQOutput\"\n",
    "    },\n",
    "    \"CheckDQOutput\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n",
    "      \"OutputPath\": \"$.Payload\",\n",
    "      \"Parameters\": {\n",
    "        \"Payload.$\": \"$\",\n",
    "        \"FunctionName\": \"<ARN_DQ_CHECK_LAMBDA_FUNCTION>\"\n",
    "      },\n",
    "      \"Retry\": [\n",
    "        {\n",
    "          \"ErrorEquals\": [\n",
    "            \"Lambda.ServiceException\",\n",
    "            \"Lambda.AWSLambdaException\",\n",
    "            \"Lambda.SdkClientException\"\n",
    "          ],\n",
    "          \"IntervalSeconds\": 2,\n",
    "          \"MaxAttempts\": 6,\n",
    "          \"BackoffRate\": 2\n",
    "        }\n",
    "      ],\n",
    "      \"Next\": \"Choice\"\n",
    "    },\n",
    "    \"Choice\": {\n",
    "      \"Type\": \"Choice\",\n",
    "      \"Choices\": [\n",
    "        {\n",
    "          \"Not\": {\n",
    "            \"Variable\": \"$.dqstatus\",\n",
    "            \"StringEquals\": \"SUCCEEDED\"\n",
    "          },\n",
    "          \"Next\": \"NotifyDQFail\"\n",
    "        }\n",
    "      ],\n",
    "      \"Default\": \"Pass\"\n",
    "    },\n",
    "    \"NotifyDQFail\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::sns:publish\",\n",
    "      \"Parameters\": {\n",
    "        \"Message.$\": \"$\",\n",
    "        \"TopicArn\": \"<ARN_SNS_TOPIC_FOR_NOTIFICATION>\"\n",
    "      },\n",
    "      \"Next\": \"Fail\"\n",
    "    },\n",
    "    \"Fail\": {\n",
    "      \"Type\": \"Fail\",\n",
    "      \"Error\": \"Data Quality Check Failed\"\n",
    "    },\n",
    "    \"Pass\": {\n",
    "      \"Type\": \"Pass\",\n",
    "      \"End\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lambda Code to Check DQ Rules Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # TODO implement\n",
    "    bucketname = \"\"\n",
    "    filename = \"\"\n",
    "    jobname = event[\"JobName\"]\n",
    "    for o in event[\"Outputs\"]:\n",
    "        bucketname = o[\"Location\"][\"Bucket\"]\n",
    "        if \"dq-validation\" in o[\"Location\"][\"Key\"]:\n",
    "            filename = o[\"Location\"][\"Key\"]\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    content_object = s3.Object(bucketname, filename)\n",
    "    file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    profilejson = json.loads(file_content)\n",
    "    \n",
    "    ruleset = \"\"\n",
    "    status = \"\"\n",
    "    \n",
    "    for rs in profilejson[\"rulesetResults\"]:\n",
    "        ruleset = rs[\"name\"]\n",
    "        status = rs[\"status\"]\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'dqstatus': status,\n",
    "        'ruleset': ruleset,\n",
    "        'jobname' : jobname\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ETL Pipeline Workflow Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```json\n",
    "{\n",
    "  \"Comment\": \"ETL Pipeline\",\n",
    "  \"StartAt\": \"Glue StartJobRun\",\n",
    "  \"States\": {\n",
    "    \"Glue StartJobRun\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\",\n",
    "      \"Parameters\": {\n",
    "        \"JobName\": \"<GLUE_JOB_HANDLING_INGESTION\"\n",
    "      },\n",
    "      \"Next\": \"DQWorkflowCall\"\n",
    "    },\n",
    "    \"DQWorkflowCall\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::states:startExecution.sync:2\",\n",
    "      \"Parameters\": {\n",
    "        \"StateMachineArn\": \"<ARN_REUSABLE_WORKFLOW>\",\n",
    "        \"Input\": {\n",
    "          \"profilejobname\": \"<DATABREW_DATA_PROFILE_JOB_NAME>\",\n",
    "          \"AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$\": \"$$.Execution.Id\"\n",
    "        }\n",
    "      },\n",
    "      \"End\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [AWS Tutorials - Using Concurrent AWS Glue Jobs](https://www.youtube.com/watch?v=oqeiadeVEGI&list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB&index=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#========== ingestionjob code ===========\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'tablename', 'destination'])\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "customerDF = glueContext.create_dynamic_frame.from_catalog(\n",
    "             database=\"dojodb\",\n",
    "             table_name=args['tablename'], redshift_tmp_dir=\"s3://dojo-dataset/scripts/\")\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(customerDF, connection_type = \"s3\", connection_options = {\"path\": args['destination']}, format = \"csv\")\n",
    "\n",
    "#==== CLI to Run Jobs ====\n",
    "\n",
    "aws glue start-job-run --job-name ingestionjob --arguments '{\"--tablename\":\"postgres_public_customers\",\"--destination\":\"s3://dojo-dataset/customers\"}'\n",
    "\n",
    "aws glue start-job-run --job-name ingestionjob --arguments '{\"--tablename\":\"postgres_public_employees\",\"--destination\":\"s3://dojo-dataset/employees\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ETL | AWS Glue | AWS S3 | ETL Job | Detect and remediate personal identifiable information PII](https://www.youtube.com/watch?v=4ux-byYTZnA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [AWS Tutorials - Using External Libraries in AWS Glue Job](https://www.youtube.com/watch?v=8_F5nrVjOII&list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB&index=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# cleansinglib.py\n",
    "from awsglue.transforms import *\n",
    "\n",
    "def renamefield(df,oldname,newname):\n",
    "    df = RenameField.apply(df, oldname, newname) \n",
    "    return df\n",
    "```\n",
    "---\n",
    "\n",
    "```python\n",
    "# datalib.py\n",
    "def readdata(db,tbl,gc):\n",
    "    df = gc.create_dynamic_frame.from_catalog(database=db,table_name=tbl, redshift_tmp_dir=\"s3://dojo-dataset/script/\")\n",
    "    return df\n",
    "\n",
    "def writedata(df,folder,format,gc):\n",
    "    gc.write_dynamic_frame.from_options(df, connection_type = \"s3\", connection_options = {\"path\": \"s3://dojo-dataset/\" + folder}, format = format)\n",
    "```\n",
    "---\n",
    "\n",
    "```python\n",
    "# gluejobcode1.py\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "import cleansinglib\n",
    "import datalib\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "df = datalib.readdata(\"sourcedb\",\"srcpostgres_public_orderdetails\",glueContext)\n",
    "\n",
    "df = cleansinglib.renamefield(df,\"amount\",\"salesvalue\")\n",
    "\n",
    "datalib.writedata(df,\"output/orderdetails\",\"csv\",glueContext)\n",
    "```\n",
    "---\n",
    "\n",
    "```python\n",
    "# gluejobcode2.py\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "import cleansinglib\n",
    "import datalib\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "df = datalib.readdata(\"sourcedb\",\"srcpostgres_public_customers\",glueContext)\n",
    "\n",
    "datalib.writedata(df,\"output/customers\",\"json\",glueContext)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [AWS Tutorials - Using Glue Job ETL from REST API Source to Amazon S3 Bucket Destination](https://www.youtube.com/watch?v=f5Coh7C7V7I&t=153s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Build a Spark pipeline to analyze streaming data using AWS Glue, Apache Hudi, S3 and Athena](https://www.youtube.com/watch?v=uJI6B4MPmoM&t=193s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
