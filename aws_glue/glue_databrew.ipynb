{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Boto3 Documentations: GlueDataBrew](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/databrew.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import os, time, json, shutil, subprocess, zipfile\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import s3, iam, lf, glue, lambdafn, rds, dynamodb as ddb, eventbridge as event\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "ec2_client           = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource         = boto3.resource('ec2', region_name=REGION)\n",
    "dynamodb_client      = boto3.client('dynamodb')\n",
    "events_client        = boto3.client('events')\n",
    "lambda_client        = boto3.client('lambda')\n",
    "glue_client          = boto3.client('glue')\n",
    "databrew_client      = boto3.client('databrew')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create aws glue role by the name of `glue_role_name`.\n",
    "- Assign Power User Access Policy (`PowerUserAccess`) to the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_ROLE_NAME = 'glue-pipeline-role'\n",
    "DATABREW_ROLE_NAME = 'databrew-pipeline-role'\n",
    "LFN_ROLE_NAME = 'lfn-pipeline-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_arns = [\n",
    "    \"arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\",\n",
    "    \"arn:aws:iam::aws:policy/CloudWatchFullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonS3FullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\",\n",
    "    # \"arn:aws:iam::aws:policy/AdministratorAccess\",\n",
    "    # \"arn:aws:iam::aws:policy/PowerUserAccess\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "GLUE_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=GLUE_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    Description=\"Glue Service Role\"\n",
    ")['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "[iam_client.attach_role_policy(RoleName=GLUE_ROLE_NAME, PolicyArn=parn) for parn in policy_arns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_document = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"databrew.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Create the IAM role with the assume role policy document\n",
    "DATABREW_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=DATABREW_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")['Role']['Arn']\n",
    "\n",
    "\n",
    "policy_arns = [\n",
    "    'arn:aws:iam::aws:policy/service-role/AWSGlueDataBrewServiceRole',\n",
    "    'arn:aws:iam::aws:policy/AwsGlueDataBrewFullAccessPolicy',\n",
    "]\n",
    "\n",
    "[iam_client.attach_role_policy(RoleName=DATABREW_ROLE_NAME, PolicyArn=policy_arn) for policy_arn in policy_arns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\":[\n",
    "                \"ec2:DescribeSubnets\",\n",
    "                \"ec2:DescribeVpcEndpoints\",\n",
    "                \"ec2:DescribeRouteTables\",\n",
    "                \"ec2:DescribeVpcs\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"glue:*\",\n",
    "                \"databrew:*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Attach the inline policy to the role\n",
    "response = iam_client.put_role_policy(\n",
    "    RoleName=DATABREW_ROLE_NAME,\n",
    "    PolicyName='DescribeVpcNetwarkPolicy',\n",
    "    PolicyDocument=json.dumps(policy_document)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"lambda.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role with the assume role policy document\n",
    "LFN_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=LFN_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")['Role']['Arn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "[iam_client.attach_role_policy(RoleName=LFN_ROLE_NAME, PolicyArn=parn) for parn in policy_arns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Create IAM Role Policy (SQS, S3, Logs Permissions)\n",
    "# policy_document = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"s3:*\",\n",
    "#                 \"s3-object-lambda:*\"\n",
    "#             ],\n",
    "#             \"Resource\": \"*\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"logs:*\"\n",
    "#             ],\n",
    "#             \"Resource\": \"*\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# policy_name = \"s3_logs_policies\"\n",
    "\n",
    "# # Attach the inline policy to the IAM role\n",
    "# iam_client.put_role_policy(\n",
    "#     RoleName=LFN_ROLE_NAME,\n",
    "#     PolicyName=policy_name,\n",
    "#     PolicyDocument=json.dumps(policy_document)\n",
    "# )\n",
    "# print(f\"Policy {policy_name} attached to role {LFN_ROLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Bucket and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET_DATALAKE = \"httx-datalake-bkt\"\n",
    "S3_BUCKET_GLUE_ASSETS = \"httx-glue-assets-bkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders1 = ['raw/employees', 'cleansed/employees']\n",
    "folders2 = ['temporary', 'sparkHistoryLogs']\n",
    "folders3 = ['dq', 'output', 'sales']\n",
    "\n",
    "s3.create_s3_bucket(S3_BUCKET_DATALAKE, folders1+folders3)\n",
    "s3.create_s3_bucket(S3_BUCKET_GLUE_ASSETS, folders2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=S3_BUCKET_DATALAKE)\n",
    "# print(response)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f'Object: {obj[\"Key\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RDS Databases & it's Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'EmployeeDB'\n",
    "DB_USERNAME = os.environ['USERNAME']\n",
    "DB_PASSWORD = os.environ['PASSWORD']\n",
    "SUBNET_GROUP_NAME = 'httx-rds-subnet-group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the RDS subnet group\n",
    "response = rds_client.create_db_subnet_group(\n",
    "    DBSubnetGroupName=SUBNET_GROUP_NAME,\n",
    "    DBSubnetGroupDescription='Subnet group for RDS instance',\n",
    "    SubnetIds=SUBNET_IDS\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\n",
    "        'db_instance_identifier': 'httx-rds-mysql',\n",
    "        'db_name': DB_NAME,\n",
    "        'db_username': DB_USERNAME,\n",
    "        'db_password': DB_PASSWORD,\n",
    "        'engine': 'mysql',\n",
    "        'port': 3306,\n",
    "        'engine_version': '8.0.32',\n",
    "        'db_instance_class': 'db.t3.micro',\n",
    "        'allocated_storage': 20,\n",
    "        'availability_zone': 'us-east-1a',\n",
    "        'tags': [{'Key': 'Project', 'Value': 'glue-rds-Crawler'}],\n",
    "        'security_group_ids': [SECURITY_GROUP_ID],\n",
    "        'db_subnet_group_name': SUBNET_GROUP_NAME,\n",
    "    },\n",
    "    {\n",
    "        'db_instance_identifier': 'httx-rds-postgresql',\n",
    "        'db_name': DB_NAME,\n",
    "        'db_username': DB_USERNAME,\n",
    "        'db_password': DB_PASSWORD,\n",
    "        'port': 5432,\n",
    "        'engine': 'postgres',\n",
    "        'engine_version': '14.13',\n",
    "        'db_instance_class': 'db.t3.micro',\n",
    "        'allocated_storage': 20,\n",
    "        'availability_zone': 'us-east-1a',\n",
    "        'tags': [{'Key': 'Project', 'Value': 'glue-rds-Crawler'}],\n",
    "        'security_group_ids': [SECURITY_GROUP_ID],\n",
    "        'db_subnet_group_name': SUBNET_GROUP_NAME,\n",
    "    },\n",
    "    {\n",
    "        'db_instance_identifier': 'httx-rds-mssql',\n",
    "        'db_name': '',\n",
    "        'db_username': DB_USERNAME,\n",
    "        'db_password': DB_PASSWORD,\n",
    "        'port': 1433,\n",
    "        'engine': 'sqlserver-ex',\n",
    "        'engine_version': '15.00.4153.1.v1',\n",
    "        'db_instance_class': 'db.t3.micro',\n",
    "        'allocated_storage': 20,\n",
    "        'availability_zone': 'us-east-1a',\n",
    "        'tags': [{'Key': 'Project', 'Value': 'glue-rds-Crawler'}],\n",
    "        'security_group_ids': [SECURITY_GROUP_ID],\n",
    "        'db_subnet_group_name': SUBNET_GROUP_NAME,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.create_rds_instance(**instances[0])   # 'httx-rds-mysql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the RDS instance\n",
    "response = rds_client.describe_db_instances(\n",
    "    DBInstanceIdentifier=instances[0]['db_instance_identifier']\n",
    ")\n",
    "\n",
    "# Extract the instance details\n",
    "db_instances = response['DBInstances']\n",
    "if db_instances:\n",
    "    instance = db_instances[0]\n",
    "    status = instance['DBInstanceStatus']\n",
    "    \n",
    "    if status == 'available':\n",
    "        mysql_endpoint = instance['Endpoint']['Address']\n",
    "        print(f\"RDS Endpoint: {mysql_endpoint}\")\n",
    "    else:\n",
    "        print(f\"RDS instance is in {status} state, NO ENDPOINT AVAILABLE YET!!\")\n",
    "else:\n",
    "    print(\"No RDS instance found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Gateway` endpoints serve as a target for a route in your route table for traffic destined for the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPC Endpoint parameters\n",
    "VPC_ENDPOINT_TAG = 'rds-vpc-endpoint' + date.today().strftime('%Y%m%d')\n",
    "VPC_ENDPOINT_SERVICE_NAME = f\"com.amazonaws.{REGION}.s3\"\n",
    "SECURITY_GROUP_IDS = [SECURITY_GROUP_ID]  # Security group(s) associated with the endpoint\n",
    "ROUTE_TABLE_IDS = ['rtb-0ec4311296ec952f8']\n",
    "\n",
    "# Create an Interface Endpoint\n",
    "VPC_ENDPOINT_ID = ec2_client.create_vpc_endpoint(\n",
    "    VpcEndpointType='Gateway',\n",
    "    VpcId=VPC_ID,\n",
    "    ServiceName=VPC_ENDPOINT_SERVICE_NAME,\n",
    "    RouteTableIds=ROUTE_TABLE_IDS,\n",
    "    # SubnetIds=sg_id,\n",
    "    # SecurityGroupIds=security_group_ids,\n",
    "    PrivateDnsEnabled=False  # Enable private DNS to resolve service names within the VPC\n",
    ")['VpcEndpoint']['VpcEndpointId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc_endpoints = ec2_client.describe_vpc_endpoints(\n",
    "    Filters=[\n",
    "        {'Name': 'vpc-id', 'Values': [VPC_ID]},\n",
    "        {'Name': 'service-name', 'Values': [VPC_ENDPOINT_SERVICE_NAME]}\n",
    "    ]\n",
    ")\n",
    "print(vpc_endpoints['VpcEndpoints'][0]['VpcEndpointId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client.create_tags(Resources=[VPC_ENDPOINT_ID],Tags=[{'Key': 'Name', 'Value': VPC_ENDPOINT_TAG}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load sql data from Local Machine to RDS Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Load into MySQL (TESTED):\n",
    "\n",
    "    -   `$ mysql -h <rds-endpoint> -p <port> -U <username> -d <dbname>` -> Connect via Command Line if needed\n",
    "    -   `$ mysql -h {mysql_endpoint} -P {mysql_port} -u httxadmin -p'{DB_PASSWORD}' interview_questions < /Users/am/mydocs/Software_Development/Web_Development/aws/aws_rds/interview_questions.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mysql -h {mysql_endpoint} -P {instances[0]['port']} -u {DB_USERNAME} -p'{DB_PASSWORD}' {DB_NAME} < ./glue_etl_pipelines/glue_etl_pipeline/mysql_employees.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Catalog Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_DB_NAME = 'httx-catalog-db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage\n",
    "DATALAKE_LOCATION_URI = f\"s3://{S3_BUCKET_DATALAKE}\"\n",
    "\n",
    "create_database_response = glue_client.create_database(\n",
    "    CatalogId=ACCOUNT_ID,\n",
    "    DatabaseInput={\n",
    "        'Name': CATALOG_DB_NAME,\n",
    "        'Description': 'A Multi-purpose Database',\n",
    "        'LocationUri': DATALAKE_LOCATION_URI,\n",
    "    }\n",
    ")\n",
    "print(create_database_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant `CREATE_TABLE` permission on `Catalog DB` to `glue_role_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arn for glue_role_name\n",
    "lf_principle = GLUE_ROLE_ARN\n",
    "\n",
    "# Grant 'CREATE_TABLE' LF Permission to `glue_role_name` Role\n",
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\n",
    "        'DataLakePrincipalIdentifier': lf_principle\n",
    "    },\n",
    "    Resource={\n",
    "        'Database': {\n",
    "            'Name': CATALOG_DB_NAME\n",
    "        }\n",
    "    },\n",
    "    Permissions=['CREATE_TABLE', 'DROP'],\n",
    "    PermissionsWithGrantOption=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant `SELECT` permission on `Catalog DB` to `DATABREW_ROLE_NAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_principle = DATABREW_ROLE_ARN\n",
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\n",
    "        'DataLakePrincipalIdentifier': lf_principle\n",
    "    },\n",
    "    Resource={\n",
    "        'Table': {\n",
    "            'DatabaseName': f\"{CATALOG_DB_NAME}\",\n",
    "            'TableWildcard': {}\n",
    "        }\n",
    "    },\n",
    "    Permissions=['ALL'],\n",
    "    PermissionsWithGrantOption=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf.grant_table_level_permissions(GLUE_ROLE_ARN, CATALOG_DB_NAME, 'employees', ['DROP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.update_database(\n",
    "#     CatalogId=ACCOUNT_ID,\n",
    "#     Name=CATALOG_DB_NAME,\n",
    "#     DatabaseInput={\n",
    "#         'Name': CATALOG_DB_NAME,\n",
    "#         'UseOnlyIamAccessControl': False\n",
    "#     }\n",
    "# )\n",
    "# lf.register_s3_path_as_data_lake_location(LFDB_LOCATION_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawler-0**(Sources): Wait for RDS instance come into AVAILABE State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_CONNECTION_NAME = 'mysql_connection'\n",
    "MYSQL_CRAWLER_NAME = \"httx-mysqlcrawler\"\n",
    "mysql_endpoint = rds.get_rds_endpoint(instances[0]['db_instance_identifier'])\n",
    "mysql_connection_url = f\"jdbc:mysql://{mysql_endpoint}:{instances[0]['port']}/{instances[0]['db_name']}\"\n",
    "RDS_CRAWLER_TARGET_PATH = f\"{instances[0]['db_name']}/Employee\"\n",
    "SOURCE_TABLE_PREFIX = \"src_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEM_DIR = f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\"\n",
    "SPARK_EVENT_LOG_PATH = f\"s3://{S3_BUCKET_GLUE_ASSETS}/sparkHistoryLogs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.create_glue_connection(\n",
    "    MYSQL_CONNECTION_NAME, \n",
    "    mysql_connection_url, \n",
    "    DB_USERNAME, \n",
    "    DB_PASSWORD, \n",
    "    SECURITY_GROUP_ID, \n",
    "    SUBNET_ID, \n",
    "    REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.create_glue_jdbc_crawler(\n",
    "    MYSQL_CRAWLER_NAME, \n",
    "    MYSQL_CONNECTION_NAME, \n",
    "    GLUE_ROLE_ARN, \n",
    "    CATALOG_DB_NAME, \n",
    "    RDS_CRAWLER_TARGET_PATH, \n",
    "    table_prefix=SOURCE_TABLE_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name=MYSQL_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf.grant_table_level_permissions(\n",
    "#     GLUE_ROLE_ARN, \n",
    "#     CATALOG_DB_NAME, \n",
    "#     f\"{SOURCE_TABLE_PREFIX}{DB_NAME}_employee\", \n",
    "#     ['SELECT']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials - Working with AWS Glue DataBrew](https://www.youtube.com/watch?v=rHRppriCGvg&list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB&index=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [lab](https://aws-dojo.com/ws32/labs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enable Data API on RDS Instance\n",
    "- To enable the Data API for an AWS RDS MySQL instance, you need to use boto3 to modify the instance settings. The Data API is a feature specifically available for Amazon Aurora Serverless v1 clusters. If you're using MySQL on a standard RDS instance, the Data API is not supported.\n",
    "- The primary purpose of enabling Data API is to allow developers to run SQL queries on Amazon Aurora Serverless v1 databases without needing a persistent database connection. Instead, it uses HTTPS requests via an API endpoint, enabling serverless and lightweight interactions.\n",
    "- `FAILED`: Failed with RDS Data Sources\n",
    "- `SUCCEDED`: Succeed with S3 Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_SALES_CRAWLER_NAME = \"httx-s3_raw_crawler\"\n",
    "S3_CRAWLER_TARGET_PATH = f\"s3://{S3_BUCKET_DATALAKE}/{'sales'}\"\n",
    "\n",
    "sales_data_file = os.environ['DATA']+'/sales.csv'  # The local file you want to upload\n",
    "object_name1 = f\"sales/sales.csv\"                  # The name to save the file as in the S3 bucket\n",
    "s3.upload_file_to_s3(S3_BUCKET_DATALAKE, sales_data_file, object_name1)\n",
    "glue.create_glue_s3_crawler(\n",
    "    S3_SALES_CRAWLER_NAME,\n",
    "    GLUE_ROLE_ARN,\n",
    "    CATALOG_DB_NAME,\n",
    "    S3_CRAWLER_TARGET_PATH,\n",
    "    table_prefix=\"raw_\"\n",
    ")\n",
    "glue_client.start_crawler(Name=S3_SALES_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"raw_sales\"\n",
    "# TABLE_NAME = f\"{SOURCE_TABLE_PREFIX}{DB_NAME}_employee\"\n",
    "DATABREW_PROJECT_NAME = 'httx-databrew-project'  # (Optional: define a DataBrew project if you want)\n",
    "DATASET_NAME = 'databrew-sales-dataset'\n",
    "RECIPE_NAME = 'httx-databrew-project-recipe'\n",
    "DATABREW_JOB_NAME = 'httx-databrew-job'\n",
    "OUTPUT_S3_LOCATION = f\"s3://{S3_BUCKET_DATALAKE}/sales/sales\" # S3 path where the results will be stored\n",
    "print(OUTPUT_S3_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUCCESS\n",
    "response = databrew_client.create_dataset(\n",
    "    Name=DATASET_NAME,  # The name of the dataset in DataBrew\n",
    "    Input={\n",
    "        'DataCatalogInputDefinition': {  # Correct parameter for Glue Data Catalog\n",
    "            'DatabaseName': CATALOG_DB_NAME,\n",
    "            'TableName': TABLE_NAME\n",
    "        }\n",
    "    },\n",
    "    # Format='CSV',  # Output format, e.g., 'CSV', 'PARQUET'\n",
    "    # FormatOptions={\n",
    "    #     'Csv': {\n",
    "    #         'Delimiter': ',',          # Field delimiter\n",
    "    #         'HeaderRow': True          # Boolean indicating the first row is a header\n",
    "    #     }\n",
    "    # },\n",
    "    # Tags={\n",
    "    #     'Project': 'Sales_Dataset'  # Optional: Add tags for resource management\n",
    "    # }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = databrew_client.create_project(\n",
    "    Name=DATABREW_PROJECT_NAME,\n",
    "    DatasetName=DATASET_NAME,  # The dataset you created earlier\n",
    "    RoleArn=DATABREW_ROLE_ARN,  # IAM role that has permissions to access Glue Data Catalog and DataBrew\n",
    "    RecipeName=RECIPE_NAME  # Optionally, add an existing recipe here (you can leave it out if not needed)\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the project session\n",
    "databrew_client.start_project_session(Name=DATABREW_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrew_client.describe_project(Name=DATABREW_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILED\n",
    "recipe_steps = [\n",
    "    {\n",
    "        'Action': {\n",
    "            'Operation': 'CREATE_COLUMN',  # Correct operation to create a new column\n",
    "            'Parameters': {\n",
    "                'ColumnName': 'full_name',  # Name of the new column\n",
    "                'Expression': 'concat(first_name, \" \", last_name)'  # Valid DataBrew expression\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = databrew_client.create_recipe(\n",
    "    Name=RECIPE_NAME,  # Replace with your recipe name\n",
    "    Steps=recipe_steps,\n",
    "    Description='A recipe to add a full_name column by concatenating first_name and last_name'\n",
    ")\n",
    "\n",
    "print(\"Recipe created successfully:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = databrew_client.create_recipe_job(\n",
    "    Name=DATABREW_JOB_NAME,              # The name of the job\n",
    "    RoleArn=GLUE_ROLE_ARN,      # IAM role to run the job\n",
    "    DatasetName=DATASET_NAME,   # The dataset to use\n",
    "    RecipeName=RECIPE_NAME,     # The recipe to apply to the dataset\n",
    "    Output={\n",
    "        'S3': {\n",
    "            'Location': OUTPUT_S3_LOCATION,     # Output location in S3\n",
    "            'Format': 'CSV'         # Output file format (e.g., CSV)\n",
    "        }\n",
    "    },\n",
    "    MaxCapacity=2,  # Optional: Adjust the job capacity (resources) for the job\n",
    "    Timeout=60  # Optional: Timeout in minutes (default is 60 minutes)\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the created DataBrew job\n",
    "response = databrew_client.start_job_run(\n",
    "    Name=DATABREW_JOB_NAME  # The name of the job to start\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = databrew_client.describe_job_run(\n",
    "    Name=DATABREW_JOB_NAME,  # The name of the DataBrew job\n",
    "    RunId=run_id  # The job run ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials - AWS Glue Data Quality - Automated Data Quality Monitoring](https://www.youtube.com/watch?v=mmLijuT2rLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_SALES_CRAWLER_NAME = \"httx-s3_raw_crawler\"\n",
    "S3_CRAWLER_TARGET_PATH = f\"s3://{S3_BUCKET_DATALAKE}/{'sales'}\"\n",
    "\n",
    "sales_data_file = os.environ['DATA']+'/sales.csv'  # The local file you want to upload\n",
    "object_name1 = f\"sales/sales.csv\"                  # The name to save the file as in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file_to_s3(S3_BUCKET_DATALAKE, sales_data_file, object_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.create_glue_s3_crawler(\n",
    "    S3_SALES_CRAWLER_NAME,\n",
    "    GLUE_ROLE_ARN,\n",
    "    CATALOG_DB_NAME,\n",
    "    S3_CRAWLER_TARGET_PATH,\n",
    "    table_prefix=\"raw_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name=S3_SALES_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.start_data_quality_rule_recommendation_run(\n",
    "    DataSource={\n",
    "        'GlueTable': {\n",
    "            'DatabaseName': CATALOG_DB_NAME,\n",
    "            'TableName': 'raw_customers'\n",
    "        }\n",
    "    },\n",
    "    Role=GLUE_ROLE_NAME,\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=123,\n",
    "    CreatedRulesetName='customers_dq_ruleset',\n",
    "    # DataQualitySecurityConfiguration='string',\n",
    "    # ClientToken='string'\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_database(CatalogId=ACCOUNT_ID,Name=CATALOG_DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket1 = s3.Bucket(S3_BUCKET_DATALAKE)\n",
    "bucket2 = s3.Bucket(S3_BUCKET_GLUE_ASSETS)\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "bucket1.objects.all().delete()\n",
    "bucket2.objects.all().delete()\n",
    "\n",
    "# Delete all object versions (if versioning is enabled)\n",
    "# bucket1.object_versions.all().delete()\n",
    "# bucket2.object_versions.all().delete()\n",
    "\n",
    "# Finally, delete the bucket\n",
    "bucket1.delete()\n",
    "bucket2.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_client.delete_db_subnet_group(DBSubnetGroupName=SUBNET_GROUP_NAME)\n",
    "ec2_client.delete_vpc_endpoints(VpcEndpointIds=[VPC_ENDPOINT_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.delete_rds_instance(instances[0]['db_instance_identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_connection(ConnectionName=MYSQL_CONNECTION_NAME)\n",
    "glue_client.delete_crawler(Name=MYSQL_CRAWLER_NAME)\n",
    "glue_client.delete_crawler(Name=S3_RAW_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_client.delete_table(TableName=config_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client.delete_function(FunctionName=LFN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all rules associated with the given prefix\n",
    "rules = events_client.list_rules(NamePrefix=\"httx\")['Rules']\n",
    "\n",
    "# List all targates associated with each rule\n",
    "targets_list = [events_client.list_targets_by_rule(Rule=rule['Name'])['Targets'] for rule in rules]\n",
    "\n",
    "# Remove all targets associated with each rule\n",
    "[events_client.remove_targets(Rule=rule['Name'], Ids=[target['Id'] for target in targets]) for rule, targets, in zip(rules, targets_list)]\n",
    "\n",
    "# Delete all rules\n",
    "[events_client.delete_rule(Name=rule['Name']) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrew_client.delete_project(Name=DATABREW_PROJECT_NAME)\n",
    "databrew_client.delete_dataset(Name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detached policy arn:aws:iam::aws:policy/CloudWatchFullAccess from role lfn-pipeline-role\n",
      "Detached policy arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess from role lfn-pipeline-role\n",
      "Detached policy arn:aws:iam::aws:policy/AmazonS3FullAccess from role lfn-pipeline-role\n",
      "Deleted role lfn-pipeline-role\n"
     ]
    }
   ],
   "source": [
    "## DELETE IAM ROLE AT THE END AFTER DELETING ALL OTHER RESOURCES.\n",
    "iam.delete_iam_role(LFN_ROLE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_iam_role(DATABREW_ROLE_ARN)\n",
    "iam.delete_iam_role(GLUE_ROLE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
