{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### AWS Tutorials â€“ Building Glue ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "import os, time, json, shutil, subprocess, zipfile\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import s3, iam, lf, glue, lambdafn, rds, dynamodb as ddb, eventbridge as event\n",
    "\n",
    "load_dotenv(\".env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "ec2_client           = boto3.client('ec2', region_name='us-east-1')\n",
    "ec2_resource         = boto3.resource('ec2', region_name='us-east-1')\n",
    "dynamodb_client      = boto3.client('dynamodb')\n",
    "events_client        = boto3.client('events')\n",
    "lambda_client        = boto3.client('lambda')\n",
    "databrew_client      = boto3.client('databrew')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"><img src=\"./pipeline_creation_methods.png\" length=\"500p\" height=\"300p\"></img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create aws glue role by the name of `glue_role_name`.\n",
    "- Assign Power User Access Policy (`PowerUserAccess`) to the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_ROLE_NAME = 'glue-pipeline-role'\n",
    "DATABREW_ROLE_NAME = 'databrew-pipeline-role'\n",
    "LFN_ROLE_NAME = 'lfn-pipeline-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_arns = [\n",
    "    \"arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\",\n",
    "    \"arn:aws:iam::aws:policy/CloudWatchFullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonS3FullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\",\n",
    "    # \"arn:aws:iam::aws:policy/AdministratorAccess\",\n",
    "    # \"arn:aws:iam::aws:policy/PowerUserAccess\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "GLUE_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=GLUE_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    Description=\"Glue Service Role\"\n",
    ")['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "[iam_client.attach_role_policy(RoleName=GLUE_ROLE_NAME, PolicyArn=parn) for parn in policy_arns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_document = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"databrew.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# Create the IAM role with the assume role policy document\n",
    "DATABREW_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=DATABREW_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")['Role']['Arn']\n",
    "\n",
    "\n",
    "policy_arn = 'arn:aws:iam::aws:policy/AwsGlueDataBrewFullAccessPolicy'\n",
    "\n",
    "iam_client.attach_role_policy(RoleName=DATABREW_ROLE_NAME, PolicyArn=policy_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATABREW_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"lambda.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role with the assume role policy document\n",
    "LFN_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=LFN_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")['Role']['Arn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "[iam_client.attach_role_policy(RoleName=LFN_ROLE_NAME, PolicyArn=parn) for parn in policy_arns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Create IAM Role Policy (SQS, S3, Logs Permissions)\n",
    "# policy_document = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"s3:*\",\n",
    "#                 \"s3-object-lambda:*\"\n",
    "#             ],\n",
    "#             \"Resource\": \"*\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"logs:*\"\n",
    "#             ],\n",
    "#             \"Resource\": \"*\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# policy_name = \"s3_logs_policies\"\n",
    "\n",
    "# # Attach the inline policy to the IAM role\n",
    "# iam_client.put_role_policy(\n",
    "#     RoleName=LFN_ROLE_NAME,\n",
    "#     PolicyName=policy_name,\n",
    "#     PolicyDocument=json.dumps(policy_document)\n",
    "# )\n",
    "# print(f\"Policy {policy_name} attached to role {LFN_ROLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Bucket and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET_DATALAKE = \"httx-datalake-bkt\"\n",
    "S3_BUCKET_GLUE_ASSETS = \"httx-glue-assets-bkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders1 = ['raw/employees', 'cleansed/employees']\n",
    "folders2 = ['temporary', 'sparkHistoryLogs']\n",
    "\n",
    "s3.create_s3_bucket(S3_BUCKET_DATALAKE, folders1)\n",
    "s3.create_s3_bucket(S3_BUCKET_GLUE_ASSETS, folders2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=S3_BUCKET_GLUE_ASSETS)\n",
    "# print(response)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f'Object: {obj[\"Key\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RDS Databases & it's Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'EmployeeDB'\n",
    "DB_USERNAME = os.environ['USERNAME']\n",
    "DB_PASSWORD = os.environ['PASSWORD']\n",
    "SUBNET_GROUP_NAME = 'httx-rds-subnet-group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the RDS subnet group\n",
    "response = rds_client.create_db_subnet_group(\n",
    "    DBSubnetGroupName=SUBNET_GROUP_NAME,\n",
    "    DBSubnetGroupDescription='Subnet group for RDS instance',\n",
    "    SubnetIds=SUBNET_IDS\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\n",
    "        'db_instance_identifier': 'httx-rds-mysql',\n",
    "        'db_name': DB_NAME,\n",
    "        'db_username': DB_USERNAME,\n",
    "        'db_password': DB_PASSWORD,\n",
    "        'engine': 'mysql',\n",
    "        'port': 3306,\n",
    "        'engine_version': '8.0.32',\n",
    "        'db_instance_class': 'db.t3.micro',\n",
    "        'allocated_storage': 20,\n",
    "        'availability_zone': 'us-east-1a',\n",
    "        'tags': [{'Key': 'Project', 'Value': 'glue-rds-Crawler'}],\n",
    "        'security_group_ids': [SECURITY_GROUP_ID],\n",
    "        'db_subnet_group_name': SUBNET_GROUP_NAME,\n",
    "    },\n",
    "    {\n",
    "        'db_instance_identifier': 'httx-rds-postgresql',\n",
    "        'db_name': DB_NAME,\n",
    "        'db_username': DB_USERNAME,\n",
    "        'db_password': DB_PASSWORD,\n",
    "        'port': 5432,\n",
    "        'engine': 'postgres',\n",
    "        'engine_version': '14.13',\n",
    "        'db_instance_class': 'db.t3.micro',\n",
    "        'allocated_storage': 20,\n",
    "        'availability_zone': 'us-east-1a',\n",
    "        'tags': [{'Key': 'Project', 'Value': 'glue-rds-Crawler'}],\n",
    "        'security_group_ids': [SECURITY_GROUP_ID],\n",
    "        'db_subnet_group_name': SUBNET_GROUP_NAME,\n",
    "    },\n",
    "    {\n",
    "        'db_instance_identifier': 'httx-rds-mssql',\n",
    "        'db_name': '',\n",
    "        'db_username': DB_USERNAME,\n",
    "        'db_password': DB_PASSWORD,\n",
    "        'port': 1433,\n",
    "        'engine': 'sqlserver-ex',\n",
    "        'engine_version': '15.00.4153.1.v1',\n",
    "        'db_instance_class': 'db.t3.micro',\n",
    "        'allocated_storage': 20,\n",
    "        'availability_zone': 'us-east-1a',\n",
    "        'tags': [{'Key': 'Project', 'Value': 'glue-rds-Crawler'}],\n",
    "        'security_group_ids': [SECURITY_GROUP_ID],\n",
    "        'db_subnet_group_name': SUBNET_GROUP_NAME,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.create_rds_instance(**instances[0])   # 'httx-rds-mysql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the RDS instance\n",
    "response = rds_client.describe_db_instances(\n",
    "    DBInstanceIdentifier=instances[0]['db_instance_identifier']\n",
    ")\n",
    "\n",
    "# Extract the instance details\n",
    "db_instances = response['DBInstances']\n",
    "if db_instances:\n",
    "    instance = db_instances[0]\n",
    "    status = instance['DBInstanceStatus']\n",
    "    \n",
    "    if status == 'available':\n",
    "        mysql_endpoint = instance['Endpoint']['Address']\n",
    "        print(f\"RDS Endpoint: {mysql_endpoint}\")\n",
    "    else:\n",
    "        print(f\"RDS instance is in {status} state, NO ENDPOINT AVAILABLE YET!!\")\n",
    "else:\n",
    "    print(\"No RDS instance found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Gateway` endpoints serve as a target for a route in your route table for traffic destined for the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPC Endpoint parameters\n",
    "VPC_ENDPOINT_TAG = 'rds-glue-vpc-endpoint'\n",
    "VPC_ENDPOINT_SERVICE_NAME = 'com.amazonaws.us-east-1.s3'\n",
    "SECURITY_GROUP_IDS = [SECURITY_GROUP_ID]  # Security group(s) associated with the endpoint\n",
    "ROUTE_TABLE_IDS = ['rtb-0ec4311296ec952f8']\n",
    "\n",
    "# Create an Interface Endpoint\n",
    "VPC_ENDPOINT_ID = ec2_client.create_vpc_endpoint(\n",
    "    VpcEndpointType='Gateway',\n",
    "    VpcId=VPC_ID,\n",
    "    ServiceName=VPC_ENDPOINT_SERVICE_NAME,\n",
    "    RouteTableIds=ROUTE_TABLE_IDS,\n",
    "    # SubnetIds=sg_id,\n",
    "    # SecurityGroupIds=security_group_ids,\n",
    "    PrivateDnsEnabled=False  # Enable private DNS to resolve service names within the VPC\n",
    ")['VpcEndpoint']['VpcEndpointId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client.create_tags(Resources=[VPC_ENDPOINT_ID],Tags=[{'Key': 'Name', 'Value': 'rds_vpc_endpoint'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load sql data from Local Machine to RDS Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Load into MySQL (TESTED):\n",
    "\n",
    "    -   `$ mysql -h <rds-endpoint> -p <port> -U <username> -d <dbname>` -> Connect via Command Line if needed\n",
    "    -   `$ mysql -h {mysql_endpoint} -P {mysql_port} -u httxadmin -p'{DB_PASSWORD}' interview_questions < /Users/am/mydocs/Software_Development/Web_Development/aws/aws_rds/interview_questions.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mysql -h {mysql_endpoint} -P {instances[0]['port']} -u {DB_USERNAME} -p'{DB_PASSWORD}' {DB_NAME} < ./mysql_employees.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Catalog Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_DB_NAME = 'httx-catalog-db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage\n",
    "DATALAKE_LOCATION_URI = f\"s3://{S3_BUCKET_DATALAKE}\"\n",
    "\n",
    "create_database_response = glue_client.create_database(\n",
    "    CatalogId=ACCOUNT_ID,\n",
    "    DatabaseInput={\n",
    "        'Name': CATALOG_DB_NAME,\n",
    "        'Description': 'A Multi-purpose Database',\n",
    "        'LocationUri': DATALAKE_LOCATION_URI,\n",
    "    }\n",
    ")\n",
    "print(create_database_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant `CREATE_TABLE` permission on `Catalog DB` to `glue_role_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arn for glue_role_name\n",
    "lf_principle = GLUE_ROLE_ARN\n",
    "\n",
    "# Grant 'CREATE_TABLE' LF Permission to `glue_role_name` Role\n",
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\n",
    "        'DataLakePrincipalIdentifier': lf_principle\n",
    "    },\n",
    "    Resource={\n",
    "        'Database': {\n",
    "            'Name': CATALOG_DB_NAME\n",
    "        }\n",
    "    },\n",
    "    Permissions=['CREATE_TABLE', 'DROP'],\n",
    "    PermissionsWithGrantOption=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_principle = DATABREW_ROLE_ARN\n",
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\n",
    "        'DataLakePrincipalIdentifier': lf_principle\n",
    "    },\n",
    "    Resource={\n",
    "        'Table': {\n",
    "            'DatabaseName': f\"{CATALOG_DB_NAME}\",\n",
    "            'TableWildcard': {}\n",
    "        }\n",
    "    },\n",
    "    Permissions=['ALL'],\n",
    "    PermissionsWithGrantOption=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf.grant_table_level_permissions(GLUE_ROLE_ARN, CATALOG_DB_NAME, 'employees', ['DROP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.update_database(\n",
    "#     CatalogId=ACCOUNT_ID,\n",
    "#     Name=CATALOG_DB_NAME,\n",
    "#     DatabaseInput={\n",
    "#         'Name': CATALOG_DB_NAME,\n",
    "#         'UseOnlyIamAccessControl': False\n",
    "#     }\n",
    "# )\n",
    "# lf.register_s3_path_as_data_lake_location(LFDB_LOCATION_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawler-0**(Sources): Wait for RDS instance come into AVAILABE State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_CONNECTION_NAME = 'mysql_connection'\n",
    "MYSQL_CRAWLER_NAME = \"httx-mysqlcrawler\"\n",
    "mysql_endpoint = rds.get_rds_endpoint(instances[0]['db_instance_identifier'])\n",
    "mysql_connection_url = f\"jdbc:mysql://{mysql_endpoint}:{instances[0]['port']}/{instances[0]['db_name']}\"\n",
    "RDS_CRAWLER_TARGET_PATH = f\"{instances[0]['db_name']}/%\"\n",
    "SOURCE_TABLE_PREFIX = \"src_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEM_DIR = f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\"\n",
    "SPARK_EVENT_LOG_PATH = f\"s3://{S3_BUCKET_GLUE_ASSETS}/sparkHistoryLogs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.create_glue_connection(\n",
    "    MYSQL_CONNECTION_NAME, \n",
    "    mysql_connection_url, \n",
    "    DB_USERNAME, \n",
    "    DB_PASSWORD, \n",
    "    SECURITY_GROUP_ID, \n",
    "    SUBNET_ID, \n",
    "    REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.create_glue_jdbc_crawler(\n",
    "    MYSQL_CRAWLER_NAME, \n",
    "    MYSQL_CONNECTION_NAME, \n",
    "    GLUE_ROLE_ARN, \n",
    "    CATALOG_DB_NAME, \n",
    "    RDS_CRAWLER_TARGET_PATH, \n",
    "    table_prefix=SOURCE_TABLE_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name=MYSQL_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf.grant_table_level_permissions(\n",
    "#     GLUE_ROLE_ARN, \n",
    "#     CATALOG_DB_NAME, \n",
    "#     f\"{SOURCE_TABLE_PREFIX}{DB_NAME}_employee\", \n",
    "#     ['SELECT']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline_policy_doc = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"s3:GetObject\",\n",
    "#                 \"s3:PutObject\",\n",
    "#                 \"s3:DeleteObject\"\n",
    "#             ],\n",
    "#             \"Resource\": [\n",
    "#                 f\"arn:aws:s3:::{S3_BUCKET_DATALAKE}/*\",\n",
    "#                 f\"arn:aws:s3:::{S3_BUCKET_GLUE_ASSETS}/*\"\n",
    "#             ]\n",
    "#         },\n",
    "#     ]\n",
    "# }\n",
    "# iam.put_inline_role_policy(role_name=GLUE_ROLE_NAME, policy_name='s3_get_put_del', policy_document=inline_policy_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Event Based AWS Glue ETL Pipeline](https://www.youtube.com/watch?v=04BbCLDlvII&list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB&index=18&t=21s) || `SUCCESS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"><img src=\"./pipeline_architecture.png\" length=\"500p\" height=\"300p\"></img></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1 = './scripts/jb1_rds_s3_v2.py'           # The local file you want to upload\n",
    "object_name1 = f\"glues_cripts/jb1_rds_s3_v2.py\"     # The name to save the file as in the S3 bucket\n",
    "s3.upload_file_to_s3(S3_BUCKET_GLUE_ASSETS, file_name1, object_name1)\n",
    "\n",
    "file_name2 = './scripts/jb2_s3_s3_v2.py'            # The local file you want to upload\n",
    "object_name2 = f\"glues_cripts/jb2_s3_s3_v2.py\"      # The name to save the file as in the S3 bucket\n",
    "s3.upload_file_to_s3(S3_BUCKET_GLUE_ASSETS, file_name2, object_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Job 1**: Transforme data from RDS (MySQL) and load into `raw/employees` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME1 = 'jb1_rds_s3'\n",
    "JOB1_SCRIPT_LOCATION = f\"s3://{S3_BUCKET_GLUE_ASSETS}/glues_cripts/jb1_rds_s3_v2.py\"\n",
    "TARGET = f\"s3://{S3_BUCKET_DATALAKE}/raw/employees\"\n",
    "# create_glue_job(JOB_NAME1, JOB1_SCRIPT_LOCATION, GLUE_ROLE_ARN, TEM_DIR, SPARK_EVENT_LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue.start_glue_job(JOB_NAME1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parametarization of the Job [`PASSED`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ARGS = {\n",
    "    '--class': 'GlueApp',\n",
    "    '--enable-continuous-cloudwatch-log': 'true',\n",
    "    '--enable-glue-datacatalog': 'true',\n",
    "    '--enable-metrics': 'true',\n",
    "    '--enable-spark-ui': 'true',\n",
    "    '--job-bookmark-option': 'job-bookmark-enable',\n",
    "    '--job-language': 'python',\n",
    "    '--TempDir': TEM_DIR,\n",
    "    '--spark-event-logs-path': SPARK_EVENT_LOG_PATH,\n",
    "    '--extra-py-files': '', # Add S3 path containing zip file of external library your job depends on.\n",
    "    '--catalog_db_name': CATALOG_DB_NAME,\n",
    "    '--table_name': f\"{SOURCE_TABLE_PREFIX}{DB_NAME}_employee\",\n",
    "    '--target': TARGET,\n",
    "}\n",
    "\n",
    "# Create the Glue job\n",
    "response = glue_client.create_job(\n",
    "    Name=JOB_NAME1,\n",
    "    Role=GLUE_ROLE_ARN,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': JOB1_SCRIPT_LOCATION,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments=DEFAULT_ARGS,\n",
    "    MaxRetries=0,\n",
    "    Timeout=5,  # in minutes, max is 2,880 min (48 Hours)\n",
    "    GlueVersion='4.0',\n",
    "    NumberOfWorkers=2,\n",
    "    WorkerType='G.1X',  # can be 'Standard', 'G.1X', or 'G.2X'    # ExecutionClass='STANDARD',  # Default execution class for Glue jobs (can be 'STANDARD' or 'FLEX')\n",
    "    # MaxCapacity=10.0,  # Default maximum capacity for the Glue job\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_job_run(\n",
    "    JobName=JOB_NAME1,\n",
    "    Arguments={\n",
    "        '--catalog_db_name': CATALOG_DB_NAME,\n",
    "        '--table_name': f\"{SOURCE_TABLE_PREFIX}{DB_NAME}_employee\",\n",
    "        '--target': TARGET,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `NOTES`:\n",
    "    - If <b style=\"color:red\">LAUNCH ERROR</b> | File --spark-event-logs-path does not existPlease refer logs for details.\n",
    "    - THEN: Select `-spark-event-logs-path` through AWS Console!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawler 1**: Catalog Data from `raw/employees` as a table by the name `raw_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glue_s3_crawler(crawler_name, role_arn, db_name, target_path, table_prefix=''):\n",
    "    try:\n",
    "        response = glue_client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role=role_arn, # or glue_role_name\n",
    "            DatabaseName=db_name,\n",
    "            Description='Crawler for generated Sales schema',\n",
    "            Targets={\n",
    "                'S3Targets': [\n",
    "                    {\n",
    "                        'Path': target_path\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            TablePrefix=table_prefix,\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'DELETE_FROM_DATABASE'\n",
    "            },\n",
    "            RecrawlPolicy={\n",
    "                'RecrawlBehavior': 'CRAWL_EVERYTHING'\n",
    "            },\n",
    "            #,Configuration='{ \"Version\": 1.0, \"CrawlerOutput\": { \"Partitions\": { \"AddOrUpdateBehavior\": \"InheritFromTable\" } } }'\n",
    "        )\n",
    "        print(f\"Successfully created Glue crawler: {crawler_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Glue crawler {crawler_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_RAW_CRAWLER_NAME = \"httx-s3_raw_crawler\"\n",
    "S3_CRAWLER_TARGET_PATH = f\"s3://{S3_BUCKET_DATALAKE}/{'raw/employees'}\"\n",
    "glue.create_glue_s3_crawler(\n",
    "    S3_RAW_CRAWLER_NAME, \n",
    "    GLUE_ROLE_ARN, \n",
    "    CATALOG_DB_NAME, \n",
    "    S3_CRAWLER_TARGET_PATH, \n",
    "    table_prefix=\"raw_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name=S3_RAW_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**job 2**: Transforme data from `raw/employees` and load into `raw/cleansed` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME2 = 'jb2_s3_s3'\n",
    "JOB2_SCRIPT_LOCATION = f\"s3://{S3_BUCKET_GLUE_ASSETS}/glues_cripts/jb2_s3_s3_v2.py\"\n",
    "JOB2_TARGET = f\"s3://{S3_BUCKET_DATALAKE}/cleansed/employees\"\n",
    "# glue.create_glue_job(JOB_NAME2, JOB2_SCRIPT_LOCATION, GLUE_ROLE_ARN, TEM_DIR, SPARK_EVENT_LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue.start_glue_job(JOB_NAME2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parametarization of the Job [`PASSED`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ARGS = {\n",
    "    '--class': 'GlueApp',\n",
    "    '--enable-continuous-cloudwatch-log': 'true',\n",
    "    '--enable-glue-datacatalog': 'true',\n",
    "    '--enable-metrics': 'true',\n",
    "    '--enable-spark-ui': 'true',\n",
    "    '--job-bookmark-option': 'job-bookmark-enable',\n",
    "    '--job-language': 'python',\n",
    "    '--TempDir': TEM_DIR,\n",
    "    '--spark-event-logs-path': SPARK_EVENT_LOG_PATH,\n",
    "    '--extra-py-files': '', # Add S3 path containing zip file of external library your job depends on.\n",
    "    '--catalog_db_name': CATALOG_DB_NAME,\n",
    "    '--table_name': f\"raw_employees\",\n",
    "    '--target': JOB2_TARGET,\n",
    "}\n",
    "\n",
    "# Create the Glue job\n",
    "response = glue_client.create_job(\n",
    "    Name=JOB_NAME2,\n",
    "    Role=GLUE_ROLE_ARN,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': JOB2_SCRIPT_LOCATION,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments=DEFAULT_ARGS,\n",
    "    MaxRetries=0,\n",
    "    Timeout=5,  # in minutes, max is 2,880 min (48 Hours)\n",
    "    GlueVersion='4.0',\n",
    "    NumberOfWorkers=2,\n",
    "    WorkerType='G.1X',  # can be 'Standard', 'G.1X', or 'G.2X'    # ExecutionClass='STANDARD',  # Default execution class for Glue jobs (can be 'STANDARD' or 'FLEX')\n",
    "    # MaxCapacity=10.0,  # Default maximum capacity for the Glue job\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_job_run(\n",
    "    JobName=JOB_NAME2,\n",
    "    Arguments={\n",
    "        '--catalog_db_name': CATALOG_DB_NAME,\n",
    "        '--table_name': f\"raw_employees\",\n",
    "        '--target': JOB2_TARGET,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crawler 2**: Catalog Data from `cleased/employees` as a table by the name `cleansed_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_CLEANSED_CRAWLER_NAME = \"httx-s3_clensed_crawler\"\n",
    "S3_CRAWLER_TARGET_PATH = f\"s3://{S3_BUCKET_DATALAKE}/{'cleansed/employees'}\"\n",
    "glue.create_glue_s3_crawler(\n",
    "    S3_CLEANSED_CRAWLER_NAME, \n",
    "    GLUE_ROLE_ARN, \n",
    "    CATALOG_DB_NAME, \n",
    "    S3_CRAWLER_TARGET_PATH, \n",
    "    \"cleansed_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.start_crawler(Name=S3_CLEANSED_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "config_table_name = 'pipelineconfig'\n",
    "\n",
    "config_key_schema = [\n",
    "    {\n",
    "        'AttributeName': 'source',\n",
    "        'KeyType': 'HASH'  # Partition key\n",
    "    }\n",
    "]\n",
    "\n",
    "config_attribute_definitions = [\n",
    "    {\n",
    "        'AttributeName': 'source',\n",
    "        'AttributeType': 'S'  # String\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create tables\n",
    "ddb.create_dynamodb_table(config_table_name, config_key_schema, config_attribute_definitions)\n",
    "\n",
    "# Wait for tables to become active\n",
    "ddb.wait_for_table_active(config_table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put items into tables\n",
    "pipelineconfig_items = [\n",
    "    {\n",
    "        'source': {'S': JOB_NAME1},\n",
    "        'target': {'S': S3_RAW_CRAWLER_NAME},\n",
    "        'targettype': {'S': 'crawler'},\n",
    "    },\n",
    "    {\n",
    "        'source': {'S': S3_RAW_CRAWLER_NAME},\n",
    "        'target': {'S': JOB_NAME2},\n",
    "        'targettype': {'S': 'job'},\n",
    "    },\n",
    "    {\n",
    "        'source': {'S': JOB_NAME2},\n",
    "        'target': {'S': S3_CLEANSED_CRAWLER_NAME},\n",
    "        'targettype': {'S': 'crawler'},\n",
    "    },\n",
    "    {\n",
    "        'source': {'S': S3_CLEANSED_CRAWLER_NAME},\n",
    "        'target': {'S': 'None'},\n",
    "        'targettype': {'S': 'None'},\n",
    "    },\n",
    "]\n",
    "\n",
    "for pipelineconfig_item in pipelineconfig_items:\n",
    "    ddb.put_item_into_table(config_table_name, pipelineconfig_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambdafn.create_lambda_package(\"./lambdas\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFN_NAME = \"glue_lambda_handler\"\n",
    "zip_file = \"./package.zip\"  # Change this to the actual zip file path\n",
    "\n",
    "# Create Lambda function\n",
    "with open(zip_file, 'rb') as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "LAMBDA_ARN = lambda_client.create_function(\n",
    "    FunctionName=LFN_NAME,\n",
    "    Runtime='python3.9',\n",
    "    Role=LFN_ROLE_ARN,\n",
    "    Handler='etl_handler.lambda_handler',\n",
    "    Code={'ZipFile': zipped_code},\n",
    "    Timeout=120,\n",
    "    Environment={\n",
    "        'Variables': {\n",
    "            'foo': 'BAR'\n",
    "        }\n",
    "    }\n",
    ")['FunctionArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"version\": \"0\",\n",
    "    \"id\": \"5cb429f8-e404-131a-41b8-3ba1e94e550e\",\n",
    "    \"detail-type\": \"Glue Crawler State Change\",\n",
    "    \"source\": \"aws.glue\",\n",
    "    \"account\": \"381492255899\",\n",
    "    \"time\": \"2024-12-11T22:39:29Z\",\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"resources\": [],\n",
    "    \"detail\": {\n",
    "        \"tablesCreated\": \"0\",\n",
    "        \"warningMessage\": \"N/A\",\n",
    "        \"partitionsUpdated\": \"0\",\n",
    "        \"tablesUpdated\": \"1\",\n",
    "        \"message\": \"Crawler Succeeded\",\n",
    "        \"partitionsDeleted\": \"0\",\n",
    "        \"accountId\": \"381492255899\",\n",
    "        \"runningTime (sec)\": \"32\",\n",
    "        \"tablesDeleted\": \"0\",\n",
    "        \"crawlerName\": \"httx-s3_clensed_crawler\",\n",
    "        \"completionDate\": \"2024-12-11T22:39:29Z\",\n",
    "        \"state\": \"Succeeded\",\n",
    "        \"partitionsCreated\": \"0\",\n",
    "        \"cloudWatchLogLink\": \"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws-glue/crawlers;stream=httx-s3_clensed_crawler\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload = {\n",
    "#     \"detail-type\": \"Glue Crawler State Change\",\n",
    "#     \"detail\": {\"crawlerName\": \"httx-s3_clensed_crawler\",}\n",
    "# }\n",
    "\n",
    "# response = lambda_client.invoke(\n",
    "#     FunctionName=LFN_NAME,\n",
    "#     InvocationType='RequestResponse',  # 'RequestResponse' for synchronous execution\n",
    "#     Payload=json.dumps(payload)\n",
    "# )\n",
    "\n",
    "# # Read the response\n",
    "# response_payload = json.loads(response['Payload'].read())\n",
    "# print(\"Response:\")\n",
    "# print(json.dumps(response_payload, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdafn.print_latest_lambda_logs(LFN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Event Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB1_RULE_NAME = 'httx-job1-handler-rule'\n",
    "JOB2_RULE_NAME = 'httx-job2-handler-rule'\n",
    "S3_RAW_CRAWLER1_RULE_NAME = 'httx-crawler1-rule'\n",
    "S3_CLEANSED_CRAWLER2_RULE_NAME = 'httx-crawler2-rule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job1_rule_event_pattern = {\n",
    "    \"source\": [\"aws.glue\"],\n",
    "    \"detail-type\": [\"Glue Job State Change\"], # Event Type\n",
    "    \"detail\": {\n",
    "        \"jobName\": [JOB_NAME1],\n",
    "        \"state\": [\"SUCCEEDED\"] # MUST BE UPPERCASED\n",
    "    }\n",
    "}\n",
    "\n",
    "JOB1_RULE_ARN = events_client.put_rule(\n",
    "    Name=JOB1_RULE_NAME,\n",
    "    EventPattern=json.dumps(job1_rule_event_pattern),\n",
    "    State='ENABLED',\n",
    "    Description='Rule to capture AWS Glue job state changes',\n",
    ")['RuleArn']\n",
    "\n",
    "# Attach the Lambda function as a target to the EventBridge Rule\n",
    "events_client.put_targets(\n",
    "    Rule=JOB1_RULE_NAME,\n",
    "    Targets=[{\n",
    "        'Id': f\"{JOB1_RULE_NAME}_target\",\n",
    "        'Arn': LAMBDA_ARN\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job2_rule_event_pattern = {\n",
    "    \"source\": [\"aws.glue\"],\n",
    "    \"detail-type\": [\"Glue Job State Change\"], # Event Type\n",
    "    \"detail\": {\n",
    "        \"jobName\": [JOB_NAME2],\n",
    "        \"state\": [\"SUCCEEDED\"] # MUST BE UPPERCASED\n",
    "    }\n",
    "}\n",
    "\n",
    "JOB2_RULE_ARN = events_client.put_rule(\n",
    "    Name=JOB2_RULE_NAME,\n",
    "    EventPattern=json.dumps(job2_rule_event_pattern),\n",
    "    State='ENABLED',\n",
    "    Description='Rule to capture AWS Glue job state changes',\n",
    ")['RuleArn']\n",
    "\n",
    "# Attach the Lambda function as a target to the EventBridge Rule\n",
    "events_client.put_targets(\n",
    "    Rule=JOB2_RULE_NAME,\n",
    "    Targets=[{\n",
    "        'Id': f\"{JOB2_RULE_NAME}_target\",\n",
    "        'Arn': LAMBDA_ARN\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler1_rule_event_pattern = {\n",
    "  \"source\": [\"aws.glue\"],\n",
    "  \"detail-type\": [\"Glue Crawler State Change\"],\n",
    "  \"detail\": {\n",
    "    \"state\": [\"Succeeded\"],\n",
    "    \"crawlerName\":[ S3_RAW_CRAWLER_NAME]\n",
    "  }\n",
    "}\n",
    "\n",
    "# Create EventBridge Rule to catch Glue Crawler State Change events\n",
    "S3_RAW_CRAWLER1_RULE_ARN = events_client.put_rule(\n",
    "    Name=S3_RAW_CRAWLER1_RULE_NAME,\n",
    "    EventPattern=json.dumps(crawler1_rule_event_pattern),\n",
    "    State='ENABLED',\n",
    "    Description='Rule to capture AWS Glue Crawler state changes',\n",
    ")['RuleArn']\n",
    "\n",
    "# Attach the Lambda function as a target to the EventBridge Rule\n",
    "events_client.put_targets(\n",
    "    Rule=S3_RAW_CRAWLER1_RULE_NAME,\n",
    "    Targets=[{\n",
    "        'Id': f\"{S3_RAW_CRAWLER1_RULE_NAME}_target\",\n",
    "        'Arn': LAMBDA_ARN\n",
    "    }]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler2_rule_event_pattern = {\n",
    "  \"source\": [\"aws.glue\"],\n",
    "  \"detail-type\": [\"Glue Crawler State Change\"],\n",
    "  \"detail\": {\n",
    "    \"state\": [\"Succeeded\"],\n",
    "    \"crawlerName\": [S3_CLEANSED_CRAWLER_NAME]\n",
    "  }\n",
    "}\n",
    "\n",
    "# Create EventBridge Rule to catch Glue Crawler State Change events\n",
    "S3_CLEANSED_CRAWLER2_RULE_ARN = events_client.put_rule(\n",
    "    Name=S3_CLEANSED_CRAWLER2_RULE_NAME,\n",
    "    EventPattern=json.dumps(crawler2_rule_event_pattern),\n",
    "    State='ENABLED',\n",
    "    Description='Rule to capture AWS Glue Crawler state changes',\n",
    ")['RuleArn']\n",
    "\n",
    "# Attach the Lambda function as a target to the EventBridge Rule\n",
    "events_client.put_targets(\n",
    "    Rule=S3_CLEANSED_CRAWLER2_RULE_NAME,\n",
    "    Targets=[{\n",
    "        'Id': f\"{S3_CLEANSED_CRAWLER2_RULE_NAME}_target\",\n",
    "        'Arn': LAMBDA_ARN\n",
    "    }]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Lambda Permissions for Event Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LAMBDA_ARN = 'arn:aws:lambda:us-east-1:381492255899:function:glue_lambda_handler'\n",
    "# Grant EventBridge permission to invoke the Lambda function\n",
    "lambda_client.add_permission(\n",
    "    FunctionName=LAMBDA_ARN.split(\":\")[-1],\n",
    "    StatementId=f\"{JOB1_RULE_NAME}-invoke-permission\",\n",
    "    Action=\"lambda:InvokeFunction\",\n",
    "    Principal=\"events.amazonaws.com\",\n",
    "    SourceArn=JOB1_RULE_ARN\n",
    ")\n",
    "lambda_client.add_permission(\n",
    "    FunctionName=LAMBDA_ARN.split(\":\")[-1],\n",
    "    StatementId=f\"{JOB2_RULE_NAME}-invoke-permission\",\n",
    "    Action=\"lambda:InvokeFunction\",\n",
    "    Principal=\"events.amazonaws.com\",\n",
    "    SourceArn=JOB2_RULE_ARN\n",
    ")\n",
    "lambda_client.add_permission(\n",
    "    FunctionName=LAMBDA_ARN.split(\":\")[-1],\n",
    "    StatementId=f\"{S3_RAW_CRAWLER1_RULE_NAME}-invoke-permission\",\n",
    "    Action=\"lambda:InvokeFunction\",\n",
    "    Principal=\"events.amazonaws.com\",\n",
    "    SourceArn=S3_RAW_CRAWLER1_RULE_ARN\n",
    ")\n",
    "lambda_client.add_permission(\n",
    "    FunctionName=LAMBDA_ARN.split(\":\")[-1],\n",
    "    StatementId=f\"{S3_CLEANSED_CRAWLER2_RULE_NAME}-invoke-permission\",\n",
    "    Action=\"lambda:InvokeFunction\",\n",
    "    Principal=\"events.amazonaws.com\",\n",
    "    SourceArn=S3_CLEANSED_CRAWLER2_RULE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Add permissions for EventBridge to invoke the Glue Crawler\n",
    "# policy = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": \"glue:StartCrawler\",\n",
    "#             \"Resource\": f'arn:aws:glue:{ACCOUNT_ID}:crawler/{crawler_name}'\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# iam_client.put_role_policy(\n",
    "#     RoleName=role_arn.split('/')[-1],  # Extract role name from ARN\n",
    "#     PolicyName='EventBridgeTriggerGlueCrawlerPolicy',\n",
    "#     PolicyDocument=json.dumps(policy)\n",
    "# )\n",
    "\n",
    "# print(\"EventBridge rule and targets created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Step Functions Based AWS Glue ETL Pipeline](https://www.youtube.com/watch?v=xXMyqT2hDWk&list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB&index=6&t=23s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_database(CatalogId=ACCOUNT_ID,Name=CATALOG_DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket1 = s3.Bucket(S3_BUCKET_DATALAKE)\n",
    "bucket2 = s3.Bucket(S3_BUCKET_GLUE_ASSETS)\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "bucket1.objects.all().delete()\n",
    "bucket2.objects.all().delete()\n",
    "\n",
    "# Delete all object versions (if versioning is enabled)\n",
    "# bucket1.object_versions.all().delete()\n",
    "# bucket2.object_versions.all().delete()\n",
    "\n",
    "# Finally, delete the bucket\n",
    "bucket1.delete()\n",
    "bucket2.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_client.delete_db_subnet_group(DBSubnetGroupName=SUBNET_GROUP_NAME)\n",
    "ec2_client.delete_vpc_endpoints(VpcEndpointIds=[VPC_ENDPOINT_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.delete_rds_instance(instances[0]['db_instance_identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_connection(ConnectionName=MYSQL_CONNECTION_NAME)\n",
    "glue_client.delete_crawler(Name=MYSQL_CRAWLER_NAME)\n",
    "glue_client.delete_crawler(Name=S3_RAW_CRAWLER_NAME)\n",
    "glue_client.delete_crawler(Name=S3_CLEANSED_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_job(JobName=JOB_NAME1)\n",
    "glue_client.delete_job(JobName=JOB_NAME2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_client.delete_table(TableName=config_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client.delete_function(FunctionName=LFN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all rules associated with the given prefix\n",
    "rules = events_client.list_rules(NamePrefix=\"httx\")['Rules']\n",
    "\n",
    "# List all targates associated with each rule\n",
    "targets_list = [events_client.list_targets_by_rule(Rule=rule['Name'])['Targets'] for rule in rules]\n",
    "\n",
    "# Remove all targets associated with each rule\n",
    "[events_client.remove_targets(Rule=rule['Name'], Ids=[target['Id'] for target in targets]) for rule, targets, in zip(rules, targets_list)]\n",
    "\n",
    "# Delete all rules\n",
    "[events_client.delete_rule(Name=rule['Name']) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrew_client.delete_project(Name=DATABREW_PROJECT_NAME)\n",
    "databrew_client.delete_dataset(Name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE IAM ROLE AT THE END AFTER DELETING ALL OTHER RESOURCES.\n",
    "iam.delete_iam_role(GLUE_ROLE_NAME)\n",
    "iam.delete_iam_role(LFN_ROLE_NAME)\n",
    "iam.delete_iam_role(DATABREW_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
