{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import os, time, json, io, zipfile, requests\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import iam, s3, lf, rds, vpc, ec2\n",
    "\n",
    "from ec2 import ALL_IN_ONE_INBOUND_RULES, ALL_IN_ONE_OUTBOUND_RULES\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "# boto3.setup_default_session(profile_name=\"AMominNJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "stepfunctions_client = boto3.client('stepfunctions')\n",
    "apigateway_client    = boto3.client('apigateway')\n",
    "lsn_client           = boto3.client('lambda')\n",
    "events_client        = boto3.client('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client   = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource = boto3.resource('ec2', region_name=REGION)\n",
    "msk_client   = boto3.client('kafka')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Boto3 Docs: AWS MSK](https://boto3.amazonaws.com/v1/documentation/api/1.35.9/reference/services/kafka.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS: Get Started with AWS MSK](https://www.youtube.com/watch?v=5WaIgJwYpS8&list=PLhr1KZpdzukd2EuSB1F9zoWMTwinTkqVn&index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./useful_commands.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS: Perform Common Operations on an Amazon MSK Cluster](https://www.youtube.com/watch?v=AUx5x_jrX6I&list=PLhr1KZpdzukd2EuSB1F9zoWMTwinTkqVn&index=2) || [`NOT TESTED YET`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Performing common operations on an Amazon MSK cluster**:  \n",
    "    - Launching and expanding clusters.  \n",
    "    - Configuring auto-scaling and security settings.  \n",
    "    - Updating cluster configurations.  \n",
    "\n",
    "- **Creating an Amazon MSK Cluster** \n",
    "  - **Quick Create Method:**: Creates a cluster with best practice settings.  \n",
    "    - Default settings include:  \n",
    "      - Apache Kafka version.  \n",
    "      - Broker type.  \n",
    "      - Amazon Elastic Block Store (EBS) storage volume.  \n",
    "\n",
    "- **Modifying Cluster Properties**  \n",
    "  - Editing Broker Storage\n",
    "  - Configuring Auto Scaling for Storage \n",
    "\n",
    "- **Expanding the Cluster**  \n",
    "  - Adding Brokers \n",
    "  - Rebalancing the Cluster\n",
    "\n",
    "- **Updating Security Settings**  \n",
    "  - Current setting: IAM role-based authentication.  \n",
    "  - Enable SASL/SCRAM authentication:  \n",
    "    - Select the checkbox and save changes.  \n",
    "    - Confirm changes in the Properties tab.  \n",
    "\n",
    "- **Creating and Applying Cluster Configurations**  \n",
    "  - `Creating Configuration`\n",
    "    - Name the configuration.  \n",
    "    - Add properties \n",
    "    - Save the configuration.  \n",
    "  - `Applying Configuration`\n",
    "    - Select the new configuration\n",
    "    - Confirm changes in the Properties tab.  \n",
    "    - Revision tracking allows future updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating an Amazon MSK Cluster - Quick Create Method\n",
    "def create_msk_cluster(cluster_name, kafka_version, broker_instance_type, ebs_storage):\n",
    "    response = msk_client.create_cluster(\n",
    "        ClusterName=cluster_name,\n",
    "        KafkaVersion=kafka_version,\n",
    "        NumberOfBrokerNodes=3,\n",
    "        BrokerNodeGroupInfo={\n",
    "            'InstanceType': broker_instance_type,\n",
    "            'ClientSubnets': ['subnet-xxxxxx', 'subnet-yyyyyy', 'subnet-zzzzzz'],\n",
    "            'StorageInfo': {\n",
    "                'EbsStorageInfo': {\n",
    "                    'VolumeSize': ebs_storage\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(\"Cluster creation initiated:\", response)\n",
    "    return response['ClusterArn']\n",
    "\n",
    "# 2. Modifying Cluster Properties\n",
    "def update_broker_storage(cluster_arn, new_storage):\n",
    "    response = msk_client.update_broker_storage(\n",
    "        ClusterArn=cluster_arn,\n",
    "        CurrentVersion=msk_client.describe_cluster(ClusterArn=cluster_arn)['ClusterInfo']['CurrentVersion'],\n",
    "        TargetBrokerEBSVolumeInfo=[\n",
    "            {\n",
    "                'KafkaBrokerNodeId': broker['BrokerNodeInfo']['BrokerId'],\n",
    "                'VolumeSizeGB': new_storage\n",
    "            }\n",
    "            for broker in msk_client.list_nodes(ClusterArn=cluster_arn)['NodeInfoList']\n",
    "        ]\n",
    "    )\n",
    "    print(\"Broker storage updated:\", response)\n",
    "\n",
    "def configure_auto_scaling(cluster_arn, max_storage, target_utilization):\n",
    "    response = msk_client.update_broker_count(\n",
    "        ClusterArn=cluster_arn,\n",
    "        CurrentVersion=msk_client.describe_cluster(ClusterArn=cluster_arn)['ClusterInfo']['CurrentVersion'],\n",
    "        TargetBrokerCount=3,\n",
    "        EnhancedMonitoring='PER_TOPIC_PER_PARTITION'\n",
    "    )\n",
    "    print(\"Auto-scaling configured:\", response)\n",
    "\n",
    "# 3. Expanding the Cluster\n",
    "def add_brokers(cluster_arn, additional_brokers):\n",
    "    response = msk_client.update_broker_count(\n",
    "        ClusterArn=cluster_arn,\n",
    "        CurrentVersion=msk_client.describe_cluster(ClusterArn=cluster_arn)['ClusterInfo']['CurrentVersion'],\n",
    "        TargetNumberOfBrokers=len(additional_brokers)\n",
    "    )\n",
    "    print(\"Brokers added:\", response)\n",
    "\n",
    "def rebalance_cluster():\n",
    "    # Implement logic to rebalance cluster partitions\n",
    "    print(\"Rebalancing the cluster is a manual process using Kafka tools.\")\n",
    "\n",
    "# 4. Updating Security Settings\n",
    "def update_security_settings(cluster_arn):\n",
    "    response = msk_client.update_security(\n",
    "        ClusterArn=cluster_arn,\n",
    "        CurrentVersion=msk_client.describe_cluster(ClusterArn=cluster_arn)['ClusterInfo']['CurrentVersion'],\n",
    "        SaslScramEnabled=True\n",
    "    )\n",
    "    print(\"Security settings updated:\", response)\n",
    "\n",
    "# 5. Creating and Applying Cluster Configurations\n",
    "def create_cluster_configuration(configuration_name, log_retention_hours, auto_create_topics):\n",
    "    response = msk_client.create_configuration(\n",
    "        Name=configuration_name,\n",
    "        KafkaVersions=['2.8.1'],\n",
    "        ServerProperties=f\"log.retention.hours={log_retention_hours}\\nauto.create.topics.enable={auto_create_topics}\".encode()\n",
    "    )\n",
    "    print(\"Cluster configuration created:\", response)\n",
    "    return response['Arn']\n",
    "\n",
    "def apply_cluster_configuration(cluster_arn, config_arn):\n",
    "    response = msk_client.update_cluster_configuration(\n",
    "        ClusterArn=cluster_arn,\n",
    "        CurrentVersion=msk_client.describe_cluster(ClusterArn=cluster_arn)['ClusterInfo']['CurrentVersion'],\n",
    "        ConfigurationInfo={\n",
    "            'Arn': config_arn,\n",
    "            'Revision': 1\n",
    "        }\n",
    "    )\n",
    "    print(\"Cluster configuration applied:\", response)\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    cluster_name = \"MyMSKCluster\"\n",
    "    kafka_version = \"2.8.1\"\n",
    "    broker_instance_type = \"kafka.m5.large\"\n",
    "    ebs_storage = 100\n",
    "\n",
    "    # Step 1: Create MSK cluster\n",
    "    cluster_arn = create_msk_cluster(cluster_name, kafka_version, broker_instance_type, ebs_storage)\n",
    "    time.sleep(900)  # Wait for the cluster to be created\n",
    "\n",
    "    # Step 2: Modify cluster properties\n",
    "    update_broker_storage(cluster_arn, 500)\n",
    "    configure_auto_scaling(cluster_arn, 1000, 60)\n",
    "\n",
    "    # Step 3: Expand the cluster\n",
    "    add_brokers(cluster_arn, [\"broker-4\", \"broker-5\", \"broker-6\"])\n",
    "    rebalance_cluster()\n",
    "\n",
    "    # Step 4: Update security settings\n",
    "    update_security_settings(cluster_arn)\n",
    "\n",
    "    # Step 5: Create and apply configuration\n",
    "    config_arn = create_cluster_configuration(\"MyClusterConfig\", 72, True)\n",
    "    apply_cluster_configuration(cluster_arn, config_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Get Started with MSK Serverless](https://www.youtube.com/watch?v=Ask0ajHnDgc&list=PLhr1KZpdzukd2EuSB1F9zoWMTwinTkqVn&index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction to MSK Serverless\n",
    "- **MSK Serverless Overview**:\n",
    "  - Automatically provisions and scales compute and storage resources.\n",
    "  - Enables on-demand Apache Kafka usage with pay-per-data-streamed-and-retained pricing.\n",
    "  - Integrates with existing Kafka applications.\n",
    "  - Reduces operational overhead by eliminating cluster infrastructure management.\n",
    "  - Cost-effective for highly variable workloads with on-demand streaming capacity.\n",
    "- **Performance**:\n",
    "  - Sustains up to 200 MB/s write capacity per cluster.\n",
    "  - Supports up to 400 MB/s read capacity per cluster.\n",
    "  - Scaling through partitions:\n",
    "    - Each partition: 5 MB/s write capacity and 10 MB/s read capacity.\n",
    "- **Pricing**:\n",
    "  - Fixed cost: $0.75 per hour.\n",
    "  - Throughput-based pricing for data and partitions.\n",
    "  - Storage costs vary based on throughput and retention plans.\n",
    "- **Key Features**:\n",
    "  - Auto partition placement.\n",
    "  - Full Apache Kafka compatibility.\n",
    "  - High availability:\n",
    "    - Partitions distributed across three availability zones.\n",
    "  - Security:\n",
    "    - Private and secure connectivity via Amazon MSK VPC.\n",
    "    - IAM role-based authentication.\n",
    "\n",
    "---\n",
    "\n",
    "##### Creating an MSK Serverless Cluster\n",
    "- **Cluster Creation**:\n",
    "  - Navigate to MSK and select \"Custom Create\" method.\n",
    "  - Provide:\n",
    "    - Cluster name.\n",
    "    - Default throughput and storage settings.\n",
    "  - Networking information:\n",
    "    - Create up to five VPC configurations (one configuration for the example).\n",
    "    - Choose a VPC and provide subnets for at least two availability zones.\n",
    "  - Security settings:\n",
    "    - Retain default security group.\n",
    "    - Use IAM role-based authentication.\n",
    "  - Optional: Add custom tags.\n",
    "  - Review settings and create the cluster.\n",
    "  - Cluster creation time: ~5 minutes.\n",
    "\n",
    "---\n",
    "\n",
    "##### Using MSK Serverless with Producers and Consumers\n",
    "- **Integrated Development Environment (IDE)**:\n",
    "  - Example IDE: AWS Cloud9 (other IDEs supported).\n",
    "  - Set up producers and consumers in separate windows:\n",
    "    - Left window: Produce data to the cluster.\n",
    "    - Right window: Consume data from the cluster in real-time.\n",
    "- **Provisioned Cluster**:\n",
    "  - Set environment variable `BS` (bootstrap string) with the private endpoint.\n",
    "  - Use the `echo` command to confirm the variable is set.\n",
    "  - Create a Kafka topic:\n",
    "    - Copy and modify the topic creation command from the MSK Developer Guide.\n",
    "    - Run the command (ignore errors for existing topics).\n",
    "    - List existing topics.\n",
    "  - Start Kafka console producer and consumer:\n",
    "    - Copy and run respective commands from the MSK Developer Guide.\n",
    "    - Verify messages are transmitted between producer and consumer.\n",
    "- **Serverless Cluster**:\n",
    "  - Update the `BS` variable with the serverless cluster's bootstrap endpoint.\n",
    "  - Confirm variable changes with the `echo` command.\n",
    "  - Create a Kafka topic (same process as the provisioned cluster).\n",
    "  - Start Kafka console producer and consumer:\n",
    "    - Use the same commands with the updated `BS` variable.\n",
    "    - Verify messages sent from the producer appear in the consumer.\n",
    "\n",
    "---\n",
    "\n",
    "##### Key Demonstration Steps\n",
    "- Demonstrate data flow on:\n",
    "  - A provisioned cluster.\n",
    "  - A serverless cluster (with modified bootstrap broker endpoint).\n",
    "- Validate successful operation of Kafka producers and consumers:\n",
    "  - Text entered in the producer window appears in the consumer window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Introduction to Amazon MSK Connect](https://www.youtube.com/watch?v=KtECJViknCM&list=PLhr1KZpdzukd2EuSB1F9zoWMTwinTkqVn&index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction to Amazon MSK Connect\n",
    "- **What is Amazon MSK Connect?**  \n",
    "  - Fully managed Kafka Connect service for Amazon MSK.  \n",
    "  - Deploy source and sink connectors to move data in and out of Amazon MSK.  \n",
    "  - Create no-code data integration pipelines for various data sources and targets:  \n",
    "    - Databases (RDBMS like MySQL, Oracle, or NoSQL like MongoDB).  \n",
    "    - Key-value stores.  \n",
    "    - Search indexes.  \n",
    "    - File systems (e.g., Amazon S3).  \n",
    "- **Features**:  \n",
    "  - **Single Message Transforms (SMTs)**: Transform messages as they flow through the MSK Connect framework.  \n",
    "  - **AWS Glue Schema Registry (GSR)**: Manage schemas for serialized messages.  \n",
    "  - **IAM Integration**: Secure and manage access with AWS Identity and Access Management.  \n",
    "\n",
    "---\n",
    "\n",
    "##### Benefits of Amazon MSK Connect\n",
    "- **Traditional Kafka Connect Challenges**:  \n",
    "  - On-premises Kafka Connect requires extensive infrastructure management (hardware, OS, encryption, etc.).  \n",
    "  - Kafka Connect on Amazon EC2 reduces hardware management but still requires infrastructure deployment and configuration.  \n",
    "- **MSK Connect Advantages**:  \n",
    "  - Abstracts the complexity of infrastructure management (hardware and Kafka Connect deployment).  \n",
    "  - Focus on use cases, not maintenance.  \n",
    "  - Fully managed and serverless:\n",
    "    - No provisioning or maintaining Kafka Connect clusters.  \n",
    "    - Pay only for resources used.  \n",
    "  - Scalable throughput:  \n",
    "    - Add MSK Connect Units (MCUs) or auto-scaling policies.  \n",
    "\n",
    "---\n",
    "\n",
    "##### Use Cases for MSK Connect\n",
    "- **Data Migration**:  \n",
    "  - Migrate data from relational databases (e.g., MySQL, Oracle) or NoSQL databases (e.g., MongoDB) to Amazon S3 for:  \n",
    "    - Compliance.  \n",
    "    - Data analysis with tools like Spark Streaming, Kinesis Data Analytics, or AWS Lambda.  \n",
    "  - Use **Change Data Capture (CDC)** to stream updates, new record creation, and deletions.  \n",
    "  - Backup data to and restore from S3 for disaster recovery.  \n",
    "- **Streaming SaaS/Enterprise Data**:  \n",
    "  - Stream data from applications like Salesforce or Zendesk to data stores (e.g., Snowflake, MongoDB, Redshift) for analytics.  \n",
    "- **Kafka Migration**:  \n",
    "  - Migrate Kafka workloads and connectors from other Kafka platforms to Amazon MSK for cost efficiency and manageability.  \n",
    "\n",
    "---\n",
    "\n",
    "##### Example Architecture: Streaming Aurora MySQL to Amazon S3 Using CDC\n",
    "- **Pipeline Overview**:  \n",
    "  - Two connectors configured and deployed:  \n",
    "    1. **Source Connector** (Amazon Aurora MySQL):  \n",
    "       - Uses a Debezium connector to convert MySQL transaction logs into CDC events.  \n",
    "       - Streams CDC events through the MSK cluster.  \n",
    "       - Serializes messages as JSON and stores schemas in the Glue Schema Registry.  \n",
    "    2. **Sink Connector** (Amazon S3):  \n",
    "       - Retrieves schemas from the Glue Schema Registry.  \n",
    "       - Deserializes records streamed through the MSK cluster.  \n",
    "       - Batches records per partition and stores them in S3 buckets.  \n",
    "- **Key Characteristics**:  \n",
    "  - No-code and serverless integration.  \n",
    "  - Fully managed Kafka Connect clusters.  \n",
    "  - Fully compatible with Kafka Connect for easy migration of connectors.  \n",
    "\n",
    "---\n",
    "\n",
    "##### Additional Features\n",
    "- **Flexibility and Scalability**:  \n",
    "  - Seamlessly scale or auto-scale connectors for workload spikes.  \n",
    "  - Fully compatible with Kafka Connect for leveraging existing connectors.  \n",
    "- **Cost Efficiency**:  \n",
    "  - Pay only for the resources used.  \n",
    "\n",
    "---\n",
    "\n",
    "##### Resources\n",
    "- Learn more through additional documentation and links provided in the description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
