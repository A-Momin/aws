{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import os, time, json, io, zipfile\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import iam, s3, lf, rds, vpc, ec2\n",
    "\n",
    "from ec2 import ALL_IN_ONE_INBOUND_RULES,ALL_IN_ONE_OUTBOUND_RULES,tags\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "# boto3.setup_default_session(profile_name=\"AMominNJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "stepfunctions_client = boto3.client('stepfunctions')\n",
    "apigateway_client    = boto3.client('apigateway')\n",
    "lsn_client           = boto3.client('lambda')\n",
    "events_client        = boto3.client('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client   = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource = boto3.resource('ec2', region_name=REGION)\n",
    "msk_client   = boto3.client('kafka')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Boto3 Docs: AWS MSK](https://boto3.amazonaws.com/v1/documentation/api/1.35.9/reference/services/kafka.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [KnowledgeAmplifier: AWS MSK Service Setup](https://www.youtube.com/watch?v=BFKmQAafE_c&list=PLjfRmoYoxpNq-pjHW0n1AfkKNi_OOMUbi&index=1&t=685s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\">Steps</b>: Double-click here.\n",
    "\n",
    "<!--\n",
    "\n",
    "Step 1:\n",
    "-------\n",
    "Cretae VPC -- Name -- virtual-private-cloud  IPv4 CIDR -- 10.0.0.0/16\n",
    "Host address range -- 10.0.0.1 - 10.0.255.254\n",
    "\n",
    "Step 2:\n",
    "-------\n",
    "Create 2 public subnets \n",
    "Public-Subnet-A--10.0.0.0/24\n",
    "Host address range -- 10.0.0.1 - 10.0.0.254\n",
    "\n",
    "Public-Subnet-B--10.0.1.0/24\n",
    "Host address range -- 10.0.1.1 - 10.0.1.254\n",
    "\n",
    "Step 3:\n",
    "-------\n",
    "Check the default route table -- you will see the above 2 subnets have not been explicitly associated with any route tables and are therefore associated with the main route table.\n",
    "\n",
    "Step 4:\n",
    "-------\n",
    "Create a IGW & connect with VPC\n",
    "\n",
    "Step 5:\n",
    "-------\n",
    "Add the IGW in default route table\n",
    "\n",
    "\n",
    "Step 6:\n",
    "-------\n",
    "Launch MSK Cluster with vpc you created , unauthorised access allowed , plaintext enxryption\n",
    "(keep security group as it is)\n",
    "\n",
    "Step 7:\n",
    "-------\n",
    "Launch Linux EC2\n",
    "In the list Network choose the VPC previously created.\n",
    "In the list Auto-assign Public IP, choose Enable.\n",
    "\n",
    "Step 8:\n",
    "-------\n",
    "Once the client for Amazon MSK has been created, the security group rules must be configured to allow the connection between the cluster and the client that we have just created.\n",
    "\n",
    "For that , Add the security group id of ec2 to msk cluster security group all traffic\n",
    "\n",
    "Repeat these steps to add an inbound rule in the security group that corresponds to your client computer to allow it to receive traffic from the security group from the VPC. Now your client computer can communicate bidirectionally with the MSK Cluster.\n",
    "\n",
    "Once this is done, the newly created and configured client can be accessed.\n",
    "\n",
    "Step 9:\n",
    "-------\n",
    "sudo yum install java-1.8.0-openjdk\n",
    "wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz\n",
    "tar -xvf kafka_2.12-2.8.1.tgz\n",
    "cd kafka_2.12-2.8.1\n",
    "\n",
    "bin/kafka-topics.sh --create --topic demo_testing2 --bootstrap-server {Put the MSK bootstrap server URLs here} --replication-factor 1 --partitions 1\n",
    "bin/kafka-topics.sh --create --topic helloworld --bootstrap-server {Put the MSK bootstrap server URLs here}  --replication-factor 1 --partitions 1\n",
    "\n",
    "Step 10:\n",
    "--------\n",
    "Start the kafka Producer\n",
    "---------------------------\n",
    "bin/kafka-console-producer.sh --topic demo_testing2 --bootstrap-server {Put the MSK bootstrap server URLs here} \n",
    "\n",
    "In a new console start the kafka consumer--\n",
    "cd kafka_2.12-2.8.1\n",
    "bin/kafka-console-consumer.sh --topic helloworld --bootstrap-server {Put the MSK bootstrap server URLs here} \n",
    "\n",
    "Step 11:\n",
    "--------\n",
    "Install confluent kafka within kafka_2.12-2.8.1)\n",
    "wget  http://packages.confluent.io/archive/5.1/confluent-5.1.2-2.11.zip\n",
    "unzip confluent-5.1.2-2.11.zip\n",
    "\n",
    "export CONFLUENT_HOME=/home/ec2-user/kafka_2.12-2.8.1/confluent-5.1.2\n",
    "export PATH=$PATH:$CONFLUENT_HOME/bin\n",
    "(Note , if installing confluent kafka , where kafka is installed (i.e. in /home/ec2-user) , then CONFLUENT_HOME should be -- /home/ec2-user/confluent-5.1.2)\n",
    "\n",
    "Step 12:\n",
    "--------\n",
    "Change the bootstrap.servers in  confluent-5.1.2/etc/kafka-rest/kafka-rest.properties \n",
    "\n",
    "\n",
    "\n",
    "Step 13:\n",
    "--------\n",
    "Start Kafka Rest \n",
    "/home/ec2-user/kafka_2.12-2.8.1/confluent-5.1.2/bin/kafka-rest-start /home/ec2-user/kafka_2.12-2.8.1/confluent-5.1.2/etc/kafka-rest/kafka-rest.properties \n",
    "\n",
    "(Don't forget to allow all traffic to the security group of EC2 client machine)\n",
    "\n",
    "Url to post messages using Kafka rest API--\n",
    "http://{Put your cleint machine's Public IP here}:8082/topics/demo_testing2\n",
    "\n",
    "Content-Type: application/vnd.kafka.json.v2+json\n",
    "\n",
    "Sample Message:\n",
    "-------------\n",
    "{\"records\":[{\"value\":{\"name\": \"testUser\"}}]}\n",
    "\n",
    "Start consumer to see the messages:\n",
    "-----------------------------------\n",
    "cd kafka_2.12-2.8.1\n",
    "bin/kafka-console-consumer.sh --topic demo_testing2 --bootstrap-server {Put the MSK bootstrap server URLs here} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Kafka and Confluent Kafka are related but distinct in their offerings and use cases. Here's a comparison:\n",
    "\n",
    "-   **Apache Kafka**: An open-source distributed event-streaming platform maintained by the Apache Software Foundation. It's freely available and is community-driven.\n",
    "\n",
    "-   **Confluent Kafka**: Built on top of Apache Kafka by Confluent, a company founded by the creators of Kafka. Confluent enhances Kafka with additional tools, enterprise features, and managed services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMAZON_LINUX_AMI = \"ami-01816d07b1128cd2d\"   # amazon Linux 2023 AMI\n",
    "KAFKA_VERSION = '2.8.1'\n",
    "KAFKA_DIR = f\"\"\"/home/ec2-user/kafka_2.12-{KAFKA_VERSION}\"\"\"\n",
    "KAFKA_TOPIC_NAME = \"httx-msk-topic\" # demo_testing2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare VPC with its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPC_NAME = 'httx-msk-vpc'\n",
    "vpc_cidr_block = '10.0.0.0/16'\n",
    "VPC_ID = ec2_client.create_vpc(CidrBlock=vpc_cidr_block)['Vpc']['VpcId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Name tag to the VPC\n",
    "ec2_client.create_tags(Resources=[VPC_ID], Tags=[{'Key': 'Name', 'Value': VPC_NAME}])\n",
    "# ec2_client.describe_vpcs(VpcIds=[VPC_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_configs = [\n",
    "    {'cidr_block': '10.0.1.0/24', 'az': 'us-east-1a', 'tag': 'public-subnet-01'},\n",
    "    {'cidr_block': '10.0.2.0/24', 'az': 'us-east-1b', 'tag': 'public-subnet-02'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_subnet1 = ec2_resource.create_subnet(\n",
    "    CidrBlock=subnet_configs[0]['cidr_block'],\n",
    "    VpcId=VPC_ID,\n",
    "    AvailabilityZone=subnet_configs[0]['az']\n",
    ")\n",
    "ec2_client.create_tags(Resources=[public_subnet1.id],Tags=[{'Key': 'Name', 'Value': subnet_configs[0]['tag']}])\n",
    "\n",
    "public_subnet2 = ec2_resource.create_subnet(\n",
    "    CidrBlock=subnet_configs[1]['cidr_block'],\n",
    "    VpcId=VPC_ID,\n",
    "    AvailabilityZone=subnet_configs[1]['az']\n",
    ")\n",
    "ec2_client.create_tags(Resources=[public_subnet2.id],Tags=[{'Key': 'Name', 'Value': subnet_configs[1]['tag']}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(public_subnet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTES:` The subnets are associated with the main route table since we havn't explicitly associated them with any route tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Internet Gateway and attach that with VPC\n",
    "igw = ec2_resource.create_internet_gateway()\n",
    "ec2_client.attach_internet_gateway(InternetGatewayId=igw.id, VpcId=VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all route tables associated with the VPC\n",
    "route_tables = ec2_client.describe_route_tables(Filters=[{'Name': 'vpc-id','Values': [VPC_ID]}])\n",
    "# Extract route table information\n",
    "route_table_id = route_tables.get('RouteTables', [])[0]['Associations'][0]['RouteTableId']\n",
    "print(route_table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_params = {'DestinationCidrBlock': '0.0.0.0/0', 'GatewayId': igw.id}\n",
    "ec2_client.create_route(RouteTableId=route_table_id, **route_params)   # Specify the Internet Gateway ID (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy MSK Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECURITY_GROUP_ID_MSK_CLUSTER = ec2.create_security_group('MSK-CLUSTER-SG', VPC_ID)[\"GroupId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSK_CLUSTER_ARN = msk_client.create_cluster(\n",
    "    ClusterName='httx-msk',  # Replace with your desired cluster name\n",
    "    KafkaVersion='2.8.1',  # Replace with your desired Kafka version\n",
    "    NumberOfBrokerNodes=2,  # Default number of brokers is 3\n",
    "    BrokerNodeGroupInfo={\n",
    "        'BrokerAZDistribution': 'DEFAULT',  # Distribute brokers across availability zones\n",
    "        'InstanceType': 'kafka.t3.small', # Default broker instance type is 'kafka.m5.large' | ProvisionedThroughput is not supported for 'kafka.t3.small'\n",
    "        'ClientSubnets': [\n",
    "            public_subnet1.id,\n",
    "            public_subnet2.id\n",
    "        ],\n",
    "        'SecurityGroups': [\n",
    "            SECURITY_GROUP_ID_MSK_CLUSTER\n",
    "        ],\n",
    "        'StorageInfo': {\n",
    "            'EbsStorageInfo': {\n",
    "                # 'ProvisionedThroughput': {\n",
    "                #     'Enabled': True,\n",
    "                #     'VolumeThroughput': 250\n",
    "                # },\n",
    "                'VolumeSize': 10  # Default EBS volume size is 100 in GiB\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # ConfigurationInfo={\n",
    "    #     'Arn': 'string',\n",
    "    #     'Revision': 123\n",
    "    # },\n",
    "    ClientAuthentication={\n",
    "        # 'Sasl': {\n",
    "        #     'Scram': {\n",
    "        #         'Enabled': False\n",
    "        #     },\n",
    "        #     'Iam': {\n",
    "        #         'Enabled': False\n",
    "        #     }\n",
    "        # },\n",
    "        # 'Tls': {\n",
    "        #     'CertificateAuthorityArnList': [\n",
    "        #         'string',\n",
    "        #     ],\n",
    "        #     'Enabled': False\n",
    "        # },\n",
    "        'Unauthenticated': {\n",
    "            'Enabled': True # Allow unauthorized access\n",
    "        }\n",
    "    },\n",
    "    EncryptionInfo={\n",
    "        # 'EncryptionAtRest': {         # by default available\n",
    "        #     'DataVolumeKMSKeyId': 'string'\n",
    "        # },\n",
    "        'EncryptionInTransit': {\n",
    "            'ClientBroker': 'TLS_PLAINTEXT',  # Encryption between clients and brokers (default is TLS)\n",
    "            'InCluster': True\n",
    "        }\n",
    "    },\n",
    "    EnhancedMonitoring='DEFAULT',  # Monitoring level\n",
    "    OpenMonitoring={\n",
    "        'Prometheus': {\n",
    "            'JmxExporter': {\n",
    "                'EnabledInBroker': False  # Default JMX exporter configuration\n",
    "            },\n",
    "            'NodeExporter': {\n",
    "                'EnabledInBroker': False  # Default Node exporter configuration\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    LoggingInfo={\n",
    "        'BrokerLogs': {\n",
    "            'CloudWatchLogs': {\n",
    "                'Enabled': False\n",
    "            },\n",
    "            'Firehose': {\n",
    "                'Enabled': False\n",
    "            },\n",
    "            'S3': {\n",
    "                'Enabled': False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    Tags={\n",
    "        'Environment': 'httx-test-MSK'  # Add your tags here\n",
    "    }\n",
    ")['ClusterArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSK_CLUSTER_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run EC2 Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECURITY_GROUP_ID_MSK_CLIENT = ec2.create_security_group('MSK-CLIENT-SG', VPC_ID)[\"GroupId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch EC2 instance with tagging using TagSpecifications\n",
    "MSK_CLIENT_INSTANCE_ID = ec2_client.run_instances(\n",
    "    ImageId=AMAZON_LINUX_AMI,  # amazon Linux 2023 AMI\n",
    "    InstanceType='t2.micro',          #'t2.medium', 't2.micro'\n",
    "    MinCount=1,\n",
    "    MaxCount=1,\n",
    "    KeyName='AMominNJ',               # Replace with your key pair\n",
    "    TagSpecifications=[\n",
    "        {\n",
    "            'ResourceType': 'instance',\n",
    "            'Tags': [{'Key': 'Name', 'Value': 'KAFKA_CLIENT'}]\n",
    "        }\n",
    "    ],\n",
    "    BlockDeviceMappings=[\n",
    "        {\n",
    "            'DeviceName': '/dev/sda1',     # Default root volume\n",
    "            'Ebs': {\n",
    "                'VolumeSize': 10,          # Volume size in GiB\n",
    "                'VolumeType': 'gp2'        # General Purpose SSD\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    SecurityGroupIds=[SECURITY_GROUP_ID_MSK_CLIENT],\n",
    "    SubnetId=public_subnet1.id\n",
    ")['Instances'][0]['InstanceId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSK_CLIENT_INSTANCE_ID='i-0e235b083a673ab1c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start the instance\n",
    "# ec2_client.start_instances(InstanceIds=[MSK_CLIENT_INSTANCE_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2_client.describe_instances(InstanceIds=[MSK_CLIENT_INSTANCE_ID])\n",
    "# print(response)\n",
    "# print(response['Reservations'][0]['Instances'][0]['PublicDnsName'])\n",
    "# print(response['Reservations'][0]['Instances'][0]['PublicIpAddress'])\n",
    "CLIENT_PUBLIC_IP = response['Reservations'][0]['Instances'][0]['PublicIpAddress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds an inbound rule to SECURITY_GROUP_ID_MSK_CLIENT to allow all traffics from SECURITY_GROUP_ID_MSK_CLUSTER\n",
    "ec2_client.authorize_security_group_ingress(\n",
    "    GroupId=SECURITY_GROUP_ID_MSK_CLIENT,\n",
    "    IpPermissions=[\n",
    "        {\n",
    "            'IpProtocol': '-1',  # '-1' means all protocols\n",
    "            'UserIdGroupPairs': [\n",
    "                {\n",
    "                    'GroupId': SECURITY_GROUP_ID_MSK_CLUSTER,\n",
    "                    'Description': 'Allow all traffic from SECURITY_GROUP_ID_MSK_CLUSTER security group'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds an inbound rule to SECURITY_GROUP_ID_MSK_CLUSTER to allow all traffics from ALL_IN_ONE_SG\n",
    "ec2_client.authorize_security_group_ingress(\n",
    "    GroupId=SECURITY_GROUP_ID_MSK_CLUSTER,\n",
    "    IpPermissions=[\n",
    "        {\n",
    "            'IpProtocol': '-1',  # '-1' means all protocols\n",
    "            'UserIdGroupPairs': [\n",
    "                {\n",
    "                    'GroupId': SECURITY_GROUP_ID_MSK_CLIENT,\n",
    "                    'Description': 'Allow all traffic from SECURITY_GROUP_ID_MSK_CLIENT security group'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Kakfa Client Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ssh amazon_linux 'sudo yum install java-1.8.0-openjdk'\n",
    "! ssh amazon_linux 'sudo yum -y install java-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon_linux 'wget https://archive.apache.org/dist/kafka/{KAFKA_VERSION}/kafka_2.12-{KAFKA_VERSION}.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon_linux 'tar -xvf kafka_2.12-{KAFKA_VERSION}.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS_ENDPOINT=msk_client.get_bootstrap_brokers(ClusterArn=MSK_CLUSTER_ARN)['BootstrapBrokerString']\n",
    "print(BOOTSTRAP_SERVERS_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kafka Topic: `$ ssh amazon_linux`\n",
    "command1 = f\"\"\"{KAFKA_DIR}/bin/kafka-topics.sh --create --topic {KAFKA_TOPIC_NAME} --bootstrap-server {BOOTSTRAP_SERVERS_ENDPOINT} --replication-factor 1 --partitions 1\"\"\"\n",
    "print(command1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the kafka Producer: `$ ssh amazon_linux`\n",
    "command3 = f\"\"\"{KAFKA_DIR}/bin/kafka-console-producer.sh --topic {KAFKA_TOPIC_NAME} --bootstrap-server {BOOTSTRAP_SERVERS_ENDPOINT}\"\"\"\n",
    "print(command3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new console start the kafka consumer: `$ ssh amazon_linux`\n",
    "command4 = f\"\"\"{KAFKA_DIR}/bin/kafka-console-consumer.sh --topic {KAFKA_TOPIC_NAME} --bootstrap-server {BOOTSTRAP_SERVERS_ENDPOINT}\"\"\"\n",
    "print(command4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Download, Install and Configure Confluent Kafk**:\n",
    "    -   It facilitate publishing message into Kafka Topic using REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon_linux 'wget -P {KAFKA_DIR}/ http://packages.confluent.io/archive/5.1/confluent-5.1.2-2.11.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon_linux 'unzip {KAFKA_DIR}/confluent-5.1.2-2.11.zip -d {KAFKA_DIR}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "strings = r\"\"\"\n",
    "export CONFLUENT_HOME=/home/ec2-user/kafka_2.12-2.8.1/confluent-5.1.2\n",
    "export PATH=$PATH:$CONFLUENT_HOME/bin\n",
    "\"\"\"\n",
    "\n",
    "command7 = f\"\"\"echo '{strings}' | ssh amazon_linux 'cat >> /home/ec2-user/.bashrc'\"\"\"\n",
    "subprocess.run(command7, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOOTSTRAP_SERVERS_ENDPOINT.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BROCKER1_ENDPOINT=BOOTSTRAP_SERVERS_ENDPOINT.split(\",\")[0]\n",
    "BROCKER2_ENDPOINT=BOOTSTRAP_SERVERS_ENDPOINT.split(\",\")[1]\n",
    "prefix = \"bootstrap.servers=PLAINTEXT://localhost:9092\"\n",
    "replacement = f\"\"\"bootstrap.servers=PLAINTEXT://{BROCKER1_ENDPOINT},PLAINTEXT://{BROCKER2_ENDPOINT}\"\"\"\n",
    "print(replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT DONE YET\n",
    "# command = f\"\"\"sed -i '/^{prefix}/c\\{replacement}' $CONFLUENT_HOME/confluent-5.1.2/etc/kafka-rest/kafka-rest.properties\"\"\"\n",
    "# print(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Your bash commands go here\n",
    "echo \"This is a bash cell\"\n",
    "ls -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Edit `confluent-5.1.2/etc/kafka-rest/kafka-rest.properties` file\n",
    "    -   Replace `old_text` by `new_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start Kafka Rest API Server\n",
    "command4 = f\"\"\"sudo {KAFKA_DIR}/confluent-5.1.2/bin/kafka-rest-start {KAFKA_DIR}/confluent-5.1.2/etc/kafka-rest/kafka-rest.properties\"\"\"\n",
    "print(command4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_PUBLIC_IP='54.90.206.152'\n",
    "\n",
    "## Url to post messages using Kafka rest API--\n",
    "KAFKA_REST_ENDPOINT = f\"\"\"http://{CLIENT_PUBLIC_IP}:8082/topics/{KAFKA_TOPIC_NAME}\"\"\"\n",
    "\n",
    "\"\"\"Content-Type: application/vnd.kafka.json.v2+json\"\"\"\n",
    "\n",
    "# Sample Message:\n",
    "{\"records\":[{\"value\":{\"name\": \"testUser\"}}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_to_kafka_rest(endpoint: str, message: dict, headers: dict = None):\n",
    "    \"\"\"\n",
    "    Publishes a message to a Kafka topic using the Kafka REST API.\n",
    "\n",
    "    Parameters:\n",
    "    - endpoint (str): The Kafka REST endpoint, e.g., http://<CLIENT_PUBLIC_IP>:8082/topics/<KAFKA_TOPIC_NAME>\n",
    "    - message (dict): The message to publish, formatted as JSON.\n",
    "    - headers (dict): Optional headers for the HTTP request. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The response from the Kafka REST API.\n",
    "    \"\"\"\n",
    "    # Default headers\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/vnd.kafka.json.v2+json\"\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # POST request to the Kafka REST API\n",
    "        response = requests.post(endpoint, json={\"records\": [{\"value\": message}]}, headers=headers)\n",
    "        \n",
    "        # Raise an HTTPError if the response status is not successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()  # Parse JSON response\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error publishing to Kafka: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "message = {\n",
    "    \"key\": \"example-key\",\n",
    "    \"value\": {\n",
    "        \"field1\": \"value1\",\n",
    "        \"field2\": \"value2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "publish_to_kafka_rest(KAFKA_REST_ENDPOINT, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start consumer to see the messages: `$ ssh amazon_linux`\n",
    "command4 = f\"\"\"{KAFKA_DIR}/bin/kafka-console-consumer.sh --topic {KAFKA_TOPIC_NAME} --bootstrap-server {BOOTSTRAP_SERVERS_ENDPOINT}\"\"\"\n",
    "print(command4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Gateway Integration with Kafka Client [`NOT COMPLETED`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\" ><img src=\"./ApiGatewayIntegrationWithKafkaClient.png\" width=\"500\" height=\"300\" /></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.remove_all_rules(SECURITY_GROUP_ID_MSK_CLUSTER)\n",
    "ec2.remove_all_rules(SECURITY_GROUP_ID_MSK_CLIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc.delete_vpc_with_dependencies(VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_client.delete_cluster(ClusterArn=MSK_CLUSTER_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop the instance immediately after creation\n",
    "ec2_client.terminate_instances(InstanceIds=[MSK_CLIENT_INSTANCE_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Setup Amazon MSK (Kafka) as an event source for Lambda](https://www.youtube.com/watch?v=RGGLBEDUuMc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./msk_lambda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color:red\">Steps</b>: Double-click here.\n",
    "\n",
    "<!--\n",
    "\n",
    "- Install java on client machine.\n",
    "   ```\n",
    "    sudo yum -y install java-11\n",
    "   ```\n",
    "\n",
    "\n",
    "- Download Apache Kafka.\n",
    "  ```\n",
    "   wget https://archive.apache.org/dist/kafka/{YOUR MSK VERSION}/kafka_2.13-{YOUR MSK VERSION}.tgz\n",
    "  ```\n",
    "\n",
    "\n",
    "- Run the following command in the directory where you downloaded the TAR file in the previous step.\n",
    "  ```\n",
    "  tar -xzf kafka_2.13-{YOUR MSK VERSION}.tgz\n",
    "  ```\n",
    "\n",
    "\n",
    "- Go to the kafka_2.13-{YOUR MSK VERSION}/libs directory, then run the following command to download the Amazon MSK IAM JAR file.\n",
    "  ```\n",
    "  wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.1/aws-msk-iam-auth-1.1.1-all.jar\n",
    "  ```\n",
    "\n",
    "\n",
    "- Go to the kafka_2.13-{YOUR MSK VERSION}/bin directory. Copy the following property settings and paste them into a new file. Name the file client.properties and save it.\n",
    "    ```\n",
    "    security.protocol=SASL_SSL\n",
    "    sasl.mechanism=AWS_MSK_IAM\n",
    "    sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;\n",
    "    sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler\n",
    "    ```\n",
    "\n",
    "- To get the broker list, run following command:\n",
    "  ```\n",
    "  aws kafka get-bootstrap-brokers --cluster-arn CLUSTER_ARN\n",
    "  ```\n",
    "\n",
    " \n",
    "- Create the Topic, run the following command, replacing BootstrapServerString with one of the broker endpoints that you obtained in the previous step.\n",
    "  ```\n",
    "  <path-to-your-kafka-installation>/bin/kafka-topics.sh --create --bootstrap-server BootstrapServerString --command-config client.properties --replication-factor 2 --partitions 1 --topic MSKTutorialTopic\n",
    "  ```\n",
    "\n",
    " \n",
    "- Producer Command:\n",
    "  ```\n",
    "  /home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-producer.sh --broker-list BROKER_LIST --producer.config client.properties --topic MSKTutorialTopic\n",
    "  ```\n",
    "\n",
    " \n",
    "- Consumer Command:\n",
    "  ```\n",
    "  /home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server BROKER_LIST --consumer.config client.properties --topic MSKTutorialTopic --from-beginning\n",
    "  ```\n",
    "\n",
    "\n",
    "- Execution role for Lambda function : `AWSLambdaMSKExecutionrole`\n",
    "\n",
    "- **Reference documents**:\n",
    "  - [Kafka event example](https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html)\n",
    "  - [Client machine steps](https://docs.aws.amazon.com/msk/latest/developerguide/create-client-machine.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC2_POLICY = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"EC2MSKRoleId0\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"kafka-cluster:*Topic*\",\n",
    "        \"kafka-cluster:AlterGroup\",\n",
    "        \"kafka-cluster:ReadData\",\n",
    "        \"kafka-cluster:DescribeCluster\",\n",
    "        \"kafka-cluster:AlterCluster\",\n",
    "        \"kafka-cluster:DescribeTopic\",\n",
    "        \"kafka:Update*\",\n",
    "        \"kafka-cluster:DescribeGroup\",\n",
    "        \"kafka-cluster:Connect\",\n",
    "        \"kafka-cluster:WriteData\",\n",
    "        \"kafka:GetBootstrapBrokers\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "LAMBDA_POLICY = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"kafka-cluster:Connect\",\n",
    "        \"kafka-cluster:DescribeGroup\",\n",
    "        \"kafka-cluster:AlterGroup\",\n",
    "        \"kafka-cluster:DescribeTopic\",\n",
    "        \"kafka-cluster:ReadData\",\n",
    "        \"kafka-cluster:DescribeClusterDynamicConfiguration\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:kafka:ap-east-1:123456789123:cluster/demo-msk-cluster/ab7c0f32-n123-4567-8r23-12a3c04z5k8p-7\",\n",
    "        \"arn:aws:kafka:ap-east-1:123456789123:topic/demo-msk-cluster/ab7c0f32-n123-4567-8r23-12a3c04z5k8p-7/*\",\n",
    "        \"arn:aws:kafka:ap-east-1:123456789123:group/demo-msk-cluster/ab7c0f32-n123-4567-8r23-12a3c04z5k8p-7/*\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_VERSION='3.5.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon Linux 2023 AMI\n",
    "BASIC_INSTANCE_ID = ec2.run_ec2_instance(image_id='ami-01816d07b1128cd2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2_client.describe_instances(InstanceIds=[BASIC_INSTANCE_ID])\n",
    "print(response['Reservations'][0]['Instances'][0]['PublicDnsName'])\n",
    "print(response['Reservations'][0]['Instances'][0]['PublicIpAddress'])\n",
    "print(response['Reservations'][0]['Instances'][0]['InstanceId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon 'sudo yum -y install java-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon 'wget https://archive.apache.org/dist/kafka/{KAFKA_VERSION}/kafka_2.13-{KAFKA_VERSION}.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon 'tar -xzf kafka_2.13-{KAFKA_VERSION}.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon 'wget -P /home/ec2-user/kafka_2.13-{KAFKA_VERSION}/libs https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.1/aws-msk-iam-auth-1.1.1-all.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh amazon 'touch /home/ec2-user/kafka_2.13-{KAFKA_VERSION}/bin/client.properties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "strings = r\"\"\"security.protocol=SASL_SSL\n",
    "sasl.mechanism=AWS_MSK_IAM\n",
    "sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;\n",
    "sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler\"\"\"\n",
    "\n",
    "command = f\"\"\"echo '{strings}' | ssh amazon 'cat >> /home/ec2-user/kafka_2.13-{KAFKA_VERSION}/bin/client.properties'\"\"\"\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```js\n",
    "exports.handler = async (event) => {\n",
    "    for (let key in event.records) {\n",
    "      console.log('Key: ', key)\n",
    "\n",
    "      event.records[key].map((record) => {\n",
    "        console.log('Record: ', record)\n",
    "        const msg = Buffer.from(record.value, 'base64').toString()\n",
    "        console.log('Message:', msg)\n",
    "      })\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client.terminate_instances(InstanceIds=[BASIC_INSTANCE_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
