{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import os, time, json, io, zipfile\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import iam, s3, lf, rds, vpc, ec2, redshift\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "# boto3.setup_default_session(profile_name=\"AMominNJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subnet-0a972b05a5b162feb', 'subnet-0ca765b361e4cb186', 'subnet-0de97821ddb8236f7', 'subnet-0a160fbe0fcafe373', 'subnet-0980ad10eb313405b']\n"
     ]
    }
   ],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "AWS_DEFAULT_ROUTE_TABLE = os.environ['AWS_DEFAULT_ROUTE_TABLE']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "ec2_client           = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource         = boto3.resource('ec2', region_name=REGION)\n",
    "\n",
    "redshift_client      = boto3.client('redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials: Using Amazon Redshift in AWS based Data Lake](https://www.youtube.com/watch?v=Co8UpEYlZYA&list=PLO95rE9ahzRuUGYApNciILstNNJlvuc6g&index=1&t=1078s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   [lab](https://aws-dojo.com/ws30/labs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'httx-redshift-bkt'\n",
    "CATALOG_DB_NAME = 'httx-catalog-db'\n",
    "GLUE_ROLE_NAME = \"httx-glue-role\" \n",
    "REDSHIFT_ROLE_NAME = \"httx-redshift-role\" \n",
    "RS_CRAWLER_NAME = \"httx-rscrawler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 1.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 2.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 3.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 4.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 5.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 6.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 7.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 8.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 9.png\" width=\"1000px\" height=\"500px\" /></div>\n",
    "<div style=\"text-align:center\" ><img src=\"./images/screenshot 10.png\" width=\"1000px\" height=\"500px\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create IAM Role (for AWS Glue Service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create aws glue role by the name of `glue_role_name`.\n",
    "- Assign Power User Access Policy (`PowerUserAccess`) to the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "GLUE_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=GLUE_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    Description=\"Glue Service Role\"\n",
    ")['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws_glue_service_policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\"\n",
    "# admin_access_policy_arn = \"arn:aws:iam::aws:policy/AdministratorAccess\"\n",
    "power_user_access_policy_arn = \"arn:aws:iam::aws:policy/PowerUserAccess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "response = iam_client.attach_role_policy(\n",
    "    RoleName=GLUE_ROLE_NAME,\n",
    "    PolicyArn=power_user_access_policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"redshift.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "REDSHIFT_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=REDSHIFT_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    Description=\"Glue Service Role\"\n",
    ")['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws_glue_service_policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\"\n",
    "# admin_access_policy_arn = \"arn:aws:iam::aws:policy/AdministratorAccess\"\n",
    "amazon_redshift_all_commands_fullaccess = \"arn:aws:iam::aws:policy/AmazonRedshiftAllCommandsFullAccess\"\n",
    "amazon_s3_read_only_access = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "response = iam_client.attach_role_policy(\n",
    "    RoleName=REDSHIFT_ROLE_NAME,\n",
    "    PolicyArn=amazon_redshift_all_commands_fullaccess\n",
    ")\n",
    "response = iam_client.attach_role_policy(\n",
    "    RoleName=REDSHIFT_ROLE_NAME,\n",
    "    PolicyArn=amazon_s3_read_only_access\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Bucket and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,scripts,tmp = ['output', 'scripts', 'tmp']     # List of folders to create\n",
    "\n",
    "s3.create_s3_bucket(BUCKET_NAME, [output,scripts,tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFT_CLUSTER_PARAMS={\n",
    "    \"DBName\": 'httx-redshift-db',\n",
    "    \"ClusterIdentifier\": 'httx-redshift-cluster',\n",
    "    \"ClusterType\": 'single-node',\n",
    "    \"NodeType\": 'dc2.large',\n",
    "    \"MasterUsername\": os.environ['USERNAME'],\n",
    "    \"MasterUserPassword\": os.environ['PASSWORD'],\n",
    "    # \"ClusterSecurityGroups\": ['string',],\n",
    "    # \"VpcSecurityGroupIds\": ['string',],\n",
    "    # \"ClusterSubnetGroupName\": 'string',\n",
    "    # \"AvailabilityZone\": 'string',\n",
    "    \"PreferredMaintenanceWindow\": 'Mon:03:00-Mon:04:00',\n",
    "    # \"ClusterParameterGroupName\": 'string',\n",
    "    \"AutomatedSnapshotRetentionPeriod\": 7,\n",
    "    \"ManualSnapshotRetentionPeriod\": 7,\n",
    "    \"Port\": 5439,\n",
    "    # \"ClusterVersion\": 'string',\n",
    "    # \"AllowVersionUpgrade\": True,\n",
    "    \"NumberOfNodes\": 1,\n",
    "    \"PubliclyAccessible\": True,\n",
    "    \"Encrypted\": False,\n",
    "    # \"HsmClientCertificateIdentifier\": 'string',\n",
    "    # \"HsmConfigurationIdentifier\": 'string',\n",
    "    # \"ElasticIp\": 'string',\n",
    "    \"Tags\": [\n",
    "        {'Key': 'Name', 'Value': 'httx-rs-cluster'},\n",
    "        {'Key': 'Environment', 'Value': 'Dev'}\n",
    "    ],\n",
    "    # \"KmsKeyId\": 'string',\n",
    "    # \"EnhancedVpcRouting\": True,\n",
    "    # \"AdditionalInfo\": 'string',\n",
    "    # \"IamRoles\": ['string',],\n",
    "    # \"MaintenanceTrackName\": 'string',\n",
    "    # \"SnapshotScheduleIdentifier\": 'httx-snapshot-schedule',\n",
    "    # \"AvailabilityZoneRelocation\": True,\n",
    "    # \"AquaConfigurationStatus\": 'enabled',            # |'disabled'|'auto',\n",
    "    # \"DefaultIamRoleArn\": 'string',\n",
    "    # \"LoadSampleData\": 'string',\n",
    "    # \"ManageMasterPassword\": True,\n",
    "    # \"MasterPasswordSecretKmsKeyId\": 'string',\n",
    "    # \"IpAddressType\": 'string',\n",
    "    # \"MultiAZ\": True,\n",
    "    # \"RedshiftIdcApplicationArn\": 'string'\n",
    "}\n",
    "\n",
    "REDSHIFT_CLUSTER_PARAMS={\n",
    "    \"db_name\": 'httx-redshift-db',\n",
    "    \"cluster_identifier\": 'httx-redshift-cluster',\n",
    "    \"cluster_type\": 'single-node',\n",
    "    \"node_type\": 'dc2.large',\n",
    "    \"master_username\": os.environ['USERNAME'],\n",
    "    \"master_user_password\": os.environ['PASSWORD'],\n",
    "    \"preferred_maintenance_window\": 'Mon:03:00-Mon:04:00',\n",
    "    \"automated_snapshot_retention_period\": 1,\n",
    "    \"manual_snapshot_retention_period\": 1,\n",
    "    \"port\": 5439,\n",
    "    \"number_of_nodes\": 2,\n",
    "    \"publicly_accessible\": True,\n",
    "    \"encrypted\": False,\n",
    "    \"tags\": [\n",
    "        {'Key': 'Name', 'Value': 'httx-rs-cluster'},\n",
    "        {'Key': 'Environment', 'Value': 'Dev'}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **ClusterSecurityGroups (list)**:\n",
    "\n",
    "    -   A list of security groups to be associated with this cluster.\n",
    "    -   Default: The default cluster security group for Amazon Redshift.\n",
    "\n",
    "-   **VpcSecurityGroupIds (list)**:\n",
    "\n",
    "    -   A list of Virtual Private Cloud (VPC) security groups to be associated with the cluster.\n",
    "    -   Default: The default VPC security group is associated with the cluster.\n",
    "\n",
    "-   **ClusterSubnetGroupName (string)**:\n",
    "\n",
    "    -   The name of a cluster subnet group to be associated with this cluster.\n",
    "    -   If this parameter is not provided the resulting cluster will be deployed outside virtual private cloud (VPC).\n",
    "\n",
    "-   **AvailabilityZone (string)**:\n",
    "\n",
    "    -   The EC2 Availability Zone (AZ) in which you want Amazon Redshift to provision the cluster. For example, if you have several EC2 instances running in a specific Availability Zone, then you might want the cluster to be provisioned in the same zone in order to decrease network latency.\n",
    "    -   Default: A random, system-chosen Availability Zone in the region that is specified by the endpoint.\n",
    "    -   Constraint: The specified Availability Zone must be in the same region as the current endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING!\n",
    "def create_redshift_cluster_v3(\n",
    "    db_name=\"httx-redshift-db\",  # Default database name\n",
    "    cluster_identifier=\"httx-redshift-cluster\",  # Unique cluster identifier\n",
    "    cluster_type=\"multi-node\",  # Cluster type: 'single-node' or 'multi-node'\n",
    "    node_type=\"dc2.large\",  # Default node type\n",
    "    master_username=os.environ['USERNAME'],  # Default master username\n",
    "    master_user_password=os.environ['PASSWORD'],  # Default master password\n",
    "    # cluster_security_groups=[],  # List of cluster security group names\n",
    "    vpc_security_group_ids=[],  # List of VPC security group IDs\n",
    "    # cluster_subnet_group_name='',  # Subnet group name for cluster\n",
    "    # availability_zone='',  # AZ in which the cluster is created\n",
    "    preferred_maintenance_window='Mon:09:00-Mon:10:00',  # Maintenance window (e.g., \"Mon:09:00-Mon:10:00\")\n",
    "    # cluster_parameter_group_name='',  # Cluster parameter group name\n",
    "    automated_snapshot_retention_period=1,  # Retain automated snapshots (days)\n",
    "    manual_snapshot_retention_period=1,  # Retain manual snapshots (days)\n",
    "    port=5439,  # Port for database connections\n",
    "    # cluster_version='',  # Redshift engine version\n",
    "    allow_version_upgrade=True,  # Allow automatic version upgrades\n",
    "    number_of_nodes=2,  # Number of nodes (required for multi-node clusters)\n",
    "    publicly_accessible=False,  # Whether the cluster is publicly accessible\n",
    "    encrypted=False,  # Encrypt data at rest\n",
    "    # hsm_client_certificate_identifier='',  # HSM client certificate\n",
    "    # hsm_configuration_identifier='',  # HSM configuration\n",
    "    # elastic_ip='',  # Elastic IP address\n",
    "    tags=[],  # List of tags (e.g., [{'Key': 'Name', 'Value': 'MyCluster'}])\n",
    "    # kms_key_id='',  # AWS KMS key for encryption\n",
    "    enhanced_vpc_routing=False,  # Enable enhanced VPC routing\n",
    "    # additional_info='',  # Reserved for internal use\n",
    "    # iam_roles=[],  # List of IAM roles for Redshift to assume\n",
    "    # maintenance_track_name='',  # Maintenance track (e.g., 'current' or 'trailing')\n",
    "    # snapshot_schedule_identifier='',  # Snapshot schedule ID\n",
    "    availability_zone_relocation=False,  # Allow relocation across AZs\n",
    "    aqua_configuration_status=\"auto\",  # AQUA settings: 'enabled', 'disabled', 'auto'\n",
    "    # default_iam_role_arn='',  # Default IAM role ARN\n",
    "    # load_sample_data='tickit',  # Option to load sample data (e.g., 'tickit')\n",
    "    manage_master_password=False,  # Whether AWS Secrets Manager manages the master password\n",
    "    # master_password_secret_kms_key_id='',  # KMS key for encrypting Secrets Manager password\n",
    "    ip_address_type=\"ipv4\",  # IP address type ('ipv4' or others)\n",
    "    multi_az=False,  # Multi-AZ deployment\n",
    "    # redshift_idc_application_arn='',  # ARN of IDC application (if applicable)\n",
    "    ):\n",
    "    try:\n",
    "        # Initialize Redshift client\n",
    "        redshift_client = boto3.client('redshift')\n",
    "\n",
    "        # Create cluster\n",
    "        response = redshift_client.create_cluster(\n",
    "            DBName=db_name,\n",
    "            ClusterIdentifier=cluster_identifier,\n",
    "            ClusterType=cluster_type,\n",
    "            NodeType=node_type,\n",
    "            MasterUsername=master_username,\n",
    "            MasterUserPassword=master_user_password,\n",
    "            # ClusterSecurityGroups=cluster_security_groups or [], # Cannot use both cluster security groups and VPC security groups\n",
    "            VpcSecurityGroupIds=vpc_security_group_ids or [],\n",
    "            # ClusterSubnetGroupName=cluster_subnet_group_name,\n",
    "            # AvailabilityZone=availability_zone,\n",
    "            PreferredMaintenanceWindow=preferred_maintenance_window,\n",
    "            # ClusterParameterGroupName=cluster_parameter_group_name,\n",
    "            AutomatedSnapshotRetentionPeriod=automated_snapshot_retention_period,\n",
    "            ManualSnapshotRetentionPeriod=manual_snapshot_retention_period,\n",
    "            Port=port,\n",
    "            # ClusterVersion=cluster_version,\n",
    "            AllowVersionUpgrade=allow_version_upgrade,\n",
    "            NumberOfNodes=number_of_nodes if cluster_type == \"multi-node\" else 1,\n",
    "            PubliclyAccessible=publicly_accessible,\n",
    "            Encrypted=encrypted,\n",
    "            # HsmClientCertificateIdentifier=hsm_client_certificate_identifier,\n",
    "            # HsmConfigurationIdentifier=hsm_configuration_identifier,\n",
    "            # ElasticIp=elastic_ip,\n",
    "            Tags=tags or [],\n",
    "            # KmsKeyId=kms_key_id,\n",
    "            EnhancedVpcRouting=enhanced_vpc_routing,\n",
    "            # AdditionalInfo=additional_info,\n",
    "            # IamRoles=iam_roles or [],\n",
    "            # MaintenanceTrackName=maintenance_track_name,\n",
    "            # SnapshotScheduleIdentifier=snapshot_schedule_identifier,\n",
    "            AvailabilityZoneRelocation=availability_zone_relocation,\n",
    "            AquaConfigurationStatus=aqua_configuration_status,\n",
    "            # DefaultIamRoleArn=default_iam_role_arn,\n",
    "            # LoadSampleData=load_sample_data,\n",
    "            ManageMasterPassword=manage_master_password,\n",
    "            # MasterPasswordSecretKmsKeyId=master_password_secret_kms_key_id,\n",
    "            IpAddressType=ip_address_type,\n",
    "            MultiAZ=multi_az,\n",
    "            # RedshiftIdcApplicationArn=redshift_idc_application_arn,\n",
    "        )\n",
    "\n",
    "        # Return response\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Redshift cluster: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating Redshift cluster: An error occurred (UnsupportedOperation) when calling the CreateCluster operation: Dual Stack is not supported\n"
     ]
    }
   ],
   "source": [
    "create_redshift_cluster_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_redshift_cluster() got an unexpected keyword argument 'cluster_identifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mredshift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_redshift_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mREDSHIFT_CLUSTER_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_redshift_cluster() got an unexpected keyword argument 'cluster_identifier'"
     ]
    }
   ],
   "source": [
    "redshift.create_redshift_cluster(**REDSHIFT_CLUSTER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift_client.create_cluster_subnet_group(\n",
    "    ClusterSubnetGroupName='string',\n",
    "    Description='string',\n",
    "    SubnetIds=SUBNET_IDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift_client.create_cluster_security_group(\n",
    "    ClusterSecurityGroupName='string',\n",
    "    Description='string',\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'string',\n",
    "            'Value': 'string'\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift_client.describe_clusters(\n",
    "    ClusterIdentifier='string',\n",
    "    MaxRecords=123,\n",
    "    Marker='string',\n",
    "    TagKeys=[\n",
    "        'string',\n",
    "    ],\n",
    "    TagValues=[\n",
    "        'string',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Associate the IAM role with the Redshift cluster (OPTIONAL)\n",
    "# response = redshift_client.modify_cluster_iam_roles(\n",
    "#     ClusterIdentifier=cluster_identifier,\n",
    "#     AddIamRoles=[power_user_access_policy_arn]  # This adds the role to the cluster\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Private Link (VPC Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Gateway` endpoints serve as a target for a route in your route table for traffic destined for the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPC Endpoint parameters\n",
    "SERVICE_NAME = 'com.amazonaws.us-east-1.s3'  # Replace with the desired service (e.g., S3)\n",
    "ROUTE_TABLE_IDS = [AWS_DEFAULT_ROUTE_TABLE]\n",
    "\n",
    "# Create an Interface Endpoint\n",
    "vpc_endpoint_id = ec2_client.create_vpc_endpoint(\n",
    "    VpcEndpointType='Gateway',\n",
    "    VpcId=VPC_ID,\n",
    "    ServiceName=SERVICE_NAME,\n",
    "    RouteTableIds=ROUTE_TABLE_IDS,\n",
    "    # SubnetIds=SUBNET_IDS,\n",
    "    # SecurityGroupIds=[SECURITY_GROUP_ID],\n",
    "    PrivateDnsEnabled=False  # Enable private DNS to resolve service names within the VPC\n",
    ")['VpcEndpoint']['VpcEndpointId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client.create_tags(Resources=['vpc_endpoint_id'],Tags=[{'Key': 'Name', 'Value': 'rs-glue-vpc-endpoint'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the VPC Endpoint\n",
    "# response = ec2_client.delete_vpc_endpoints(\n",
    "#     VpcEndpointIds=[vpc_endpoint_id]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create glue components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Glue Catalog Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage\n",
    "datalake_location_uri = f\"s3://{bucket_name}\" #/{datalake_folder_name}\"\n",
    "\n",
    "create_database_response = glue_client.create_database(\n",
    "    CatalogId=ACCOUNT_ID,\n",
    "    DatabaseInput={\n",
    "        'Name': catalog_db_name,\n",
    "        'Description': 'This is a Glue Catalog database',\n",
    "        'LocationUri': datalake_location_uri,\n",
    "    }\n",
    ")\n",
    "print(create_database_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant `CREATE_TABLE` permission to `glue_role_name` on data catalog DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arn for glue_role_name\n",
    "lf_principle = create_role_response['Role']['Arn']\n",
    "\n",
    "# Grant 'CREATE_TABLE' LF Permission to `glue_role_name` Role\n",
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\n",
    "        'DataLakePrincipalIdentifier': lf_principle\n",
    "    },\n",
    "    Resource={\n",
    "        'Database': {\n",
    "            'Name': catalog_db_name\n",
    "        }\n",
    "    },\n",
    "    Permissions=['CREATE_TABLE', 'ALTER'],\n",
    "    PermissionsWithGrantOption=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_database(CatalogId=ACCOUNT_ID,Name=catalog_db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create JDBC connection for Glue Crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.delete_connection?\n",
    "# glue_client.get_connection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?glue_client.create_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_rs_connection_name = \"glue-rs-connection\"\n",
    "port = '5439'\n",
    "host_name = 'httx-rh-cluster.cd3k9pazjjel.us-east-1.redshift.amazonaws.com'\n",
    "\n",
    "# Construct the connection properties\n",
    "jdbc_url = f\"jdbc:redshift://{host_name}:{port}/{rs_db_name}\"\n",
    "connection_properties = {\n",
    "    'USERNAME': rs_db_username,\n",
    "    'PASSWORD': rs_db_password,\n",
    "    'JDBC_CONNECTION_URL': jdbc_url,\n",
    "    'JDBC_ENFORCE_SSL': 'false'  # set to 'true' if using SSL\n",
    "}\n",
    "\n",
    "# Construct the physical connection requirements\n",
    "physical_connection_requirements = {\n",
    "    'SecurityGroupIdList': [SECURITY_GROUP_ID],\n",
    "    'SubnetId': SUBNET_ID  # Use subnet ID instead of VPC ID\n",
    "}\n",
    "\n",
    "response = glue_client.create_connection(\n",
    "    ConnectionInput={\n",
    "        \"Name\": glue_rs_connection_name,\n",
    "        \"ConnectionType\": \"JDBC\",  # Use JDBC for Redshift\n",
    "        \"ConnectionProperties\": connection_properties,\n",
    "        \"PhysicalConnectionRequirements\": physical_connection_requirements\n",
    "    },\n",
    "    Tags={'Name': f\"{glue_rs_connection_name}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = glue_client.get_connection(Name=glue_rh_connection_name)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the Connection:\n",
    "    -   <b style=\"color:red\">FAILED</b>: For some unknown reasons connection made Using the SDK (Boto3) does not work unless you make some random eidt on the connection from AWS Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_mysql_connection_name = \"glue-mysql-connection\"\n",
    "# response = glue_client.get_connection(Name=glue_rh_connection_name)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.delete_connection(ConnectionName=glue_rh_connection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Glue Crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_crawler_response1 = glue_client.create_crawler(\n",
    "    Name=RS_CRAWLER_NAME,\n",
    "    Role=GLUE_ROLE_ARN,\n",
    "    DatabaseName=CATALOG_DB_NAME,\n",
    "    Description='Crawler for generated customer schema',\n",
    "    Targets={\n",
    "        'JdbcTargets': [\n",
    "            {\n",
    "                'ConnectionName': glue_rs_connection_name,\n",
    "                'Path': f\"{rs_db_name}/%\",\n",
    "                'Exclusions': [],  # Optional: specify any patterns to exclude\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    SchemaChangePolicy={\n",
    "        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "        'DeleteBehavior': 'DELETE_FROM_DATABASE'\n",
    "    },\n",
    "    RecrawlPolicy={\n",
    "        'RecrawlBehavior': 'CRAWL_EVERYTHING'\n",
    "    }\n",
    ")\n",
    "print(create_crawler_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_crawler_response1 = glue_client.start_crawler(Name=rds_crawler_name)\n",
    "# print(run_crawler_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?lakeformation_client.grant_permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant Table level LF permission (`SELECT`) to `glue_role_name` on the tables just created on Catalog DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\n",
    "        'DataLakePrincipalIdentifier': GLUE_ROLE_ARN\n",
    "    },\n",
    "    Resource={\n",
    "        'Table': {\n",
    "            'DatabaseName': f\"{CATALOG_DB_NAME}\",\n",
    "            'TableWildcard': {}\n",
    "        }\n",
    "    },\n",
    "    Permissions=['SELECT'],\n",
    "    PermissionsWithGrantOption=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete All Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_resources = boto3.resource('s3')\n",
    "# bucket = s3_resources.Bucket(bucket_name)\n",
    "\n",
    "# # Delete all objects in the bucket\n",
    "# bucket.objects.all().delete()\n",
    "\n",
    "# # Delete all object versions (if versioning is enabled)\n",
    "# bucket.object_versions.all().delete()\n",
    "\n",
    "# # Finally, delete the bucket\n",
    "# bucket.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete the Redshift cluster without a final snapshot\n",
    "# response = redshift_client.delete_cluster(\n",
    "#     ClusterIdentifier=cluster_identifier,\n",
    "#     SkipFinalClusterSnapshot=True  # Set to False if you want to take a final snapshot before deletion\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rds.delete_rds_instance(db_instance_identifier_mysqlrds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = glue_client.delete_connection(ConnectionName=glue_mysql_connection_name)\n",
    "# response = glue_client.delete_crawler(Name=rds_crawler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iam.delete_iam_role(glue_role_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials - Working with Data API for Amazon Redshift](https://www.youtube.com/watch?v=LFrlMQbPehA&list=PLO95rE9ahzRuUGYApNciILstNNJlvuc6g&index=5&t=13s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "redshift_client = boto3.client('redshift')\n",
    "secrets_manager_client = boto3.client('secretsmanager')\n",
    "iam_client = boto3.client('iam')\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "def create_redshift_cluster():\n",
    "    \"\"\"\n",
    "    Creates an Amazon Redshift cluster.\n",
    "    \"\"\"\n",
    "    cluster_identifier = \"dojoredshift\"\n",
    "    response = redshift_client.create_cluster(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        NodeType='dc2.large',\n",
    "        MasterUsername='awsuser',\n",
    "        MasterUserPassword='Password1!',\n",
    "        ClusterType='single-node'\n",
    "    )\n",
    "    print(f\"Creating Redshift cluster: {cluster_identifier}\")\n",
    "    return cluster_identifier\n",
    "\n",
    "def wait_for_redshift(cluster_identifier):\n",
    "    \"\"\"\n",
    "    Waits until the Redshift cluster is available.\n",
    "    \"\"\"\n",
    "    print(f\"Waiting for Redshift cluster '{cluster_identifier}' to become available...\")\n",
    "    while True:\n",
    "        response = redshift_client.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        if cluster_status == 'available':\n",
    "            print(f\"Redshift cluster '{cluster_identifier}' is available.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Cluster status: {cluster_status}\")\n",
    "            time.sleep(30)\n",
    "\n",
    "def create_secret():\n",
    "    \"\"\"\n",
    "    Creates a secret in AWS Secrets Manager for Redshift credentials.\n",
    "    \"\"\"\n",
    "    secret_name = \"dojosecret\"\n",
    "    secret_value = {\n",
    "        \"username\": \"awsuser\",\n",
    "        \"password\": \"Password1!\",\n",
    "        \"engine\": \"redshift\",\n",
    "        \"host\": \"dojoredshift.cluster-identifier.aws-region.redshift.amazonaws.com\",\n",
    "        \"port\": 5439,\n",
    "        \"dbClusterIdentifier\": \"dojoredshift\"\n",
    "    }\n",
    "    response = secrets_manager_client.create_secret(\n",
    "        Name=secret_name,\n",
    "        SecretString=str(secret_value)\n",
    "    )\n",
    "    secret_arn = response['ARN']\n",
    "    print(f\"Created secret: {secret_name}, ARN: {secret_arn}\")\n",
    "    return secret_arn\n",
    "\n",
    "def create_iam_role():\n",
    "    \"\"\"\n",
    "    Creates an IAM role for SageMaker with the necessary permissions.\n",
    "    \"\"\"\n",
    "    role_name = \"dojosagemakerrole\"\n",
    "    assume_role_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=str(assume_role_policy)\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/PowerUserAccess\"\n",
    "    )\n",
    "    print(f\"Created IAM role: {role_name}\")\n",
    "    return role_name\n",
    "\n",
    "def create_sagemaker_notebook(role_name):\n",
    "    \"\"\"\n",
    "    Creates a SageMaker notebook instance.\n",
    "    \"\"\"\n",
    "    notebook_name = \"dojodataapinotebook\"\n",
    "    response = sagemaker_client.create_notebook_instance(\n",
    "        NotebookInstanceName=notebook_name,\n",
    "        InstanceType='ml.t2.medium',\n",
    "        RoleArn=f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:role/{role_name}\"\n",
    "    )\n",
    "    print(f\"Creating SageMaker notebook: {notebook_name}\")\n",
    "    return notebook_name\n",
    "\n",
    "def main():\n",
    "    # Step 2: Launch Redshift Cluster\n",
    "    cluster_identifier = create_redshift_cluster()\n",
    "    wait_for_redshift(cluster_identifier)\n",
    "    \n",
    "    # Step 4: Configure Secrets Manager\n",
    "    secret_arn = create_secret()\n",
    "\n",
    "    # Step 5: Configure IAM Role and SageMaker Notebook\n",
    "    role_name = create_iam_role()\n",
    "    create_sagemaker_notebook(role_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials: Using Lambda UDF with Amazon Redshift](https://www.youtube.com/watch?v=HqpwL7et4eQ&list=PLO95rE9ahzRuUGYApNciILstNNJlvuc6g&index=4&t=119s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [lab](https://aws-dojo.com/excercises/excercise31/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a nested bulleted point version of the text you provided:\n",
    "\n",
    "- **Introduction**\n",
    "  - AWS Lambda can now be used to create a user-defined function (UDF) in Amazon Redshift.\n",
    "  - UDFs can be utilized in both the `SELECT` and `WHERE` clauses of SQL queries.\n",
    "  - The business logic for the UDF is defined in a Lambda function, which allows the use of programming languages like Python or Node.js.\n",
    "  \n",
    "- **Steps to Create UDF with Lambda in Redshift**\n",
    "  - **IAM Role Creation**\n",
    "    - Create an IAM role for the Redshift cluster.\n",
    "    - This IAM role should have permission to invoke the Lambda function.\n",
    "    - Redshift cluster needs permission to invoke Lambda for executing the query.\n",
    "  \n",
    "  - **Defining the External Function**\n",
    "    - Use the `CREATE EXTERNAL FUNCTION` command in Redshift.\n",
    "    - In the UDF, the Lambda function is referenced to perform the business logic.\n",
    "    - When the UDF is called in a query, it invokes the Lambda function to return the result.\n",
    "\n",
    "- **Lambda Function Structure**\n",
    "  - **Output Format**\n",
    "    - The result returned by the Lambda function must be a dictionary with four fields:\n",
    "      - `success`: Indicates if the Lambda function execution was successful (`true` or `false`).\n",
    "      - `error_message`: Contains an error description if the Lambda function fails.\n",
    "      - `result`: The actual result, which can be an array of records.\n",
    "      - `number_of_records`: Optional but recommended to indicate how many records are being returned.\n",
    "  \n",
    "  - **Return Example**\n",
    "    - Pass an array of arguments, multiply values, and append the result to an array.\n",
    "    - Return a dictionary that includes:\n",
    "      - `success` status.\n",
    "      - `result`: Array of the calculated values.\n",
    "      - `number_of_records`: Optional, but can be added.\n",
    "\n",
    "- **Creating the Lambda Function**\n",
    "  - **Lambda Creation**\n",
    "    - Create the Lambda function like any other Lambda in AWS.\n",
    "    - Ensure proper formatting when returning results as a dictionary.\n",
    "  \n",
    "  - **Returning the Output**\n",
    "    - Populate the `success` field based on exception handling.\n",
    "    - If `success` is `false`, include an `error_message`.\n",
    "    - Append results in an array and return it.\n",
    "\n",
    "- **Using the UDF in Queries**\n",
    "  - Example SQL query using the UDF:\n",
    "    - Call the UDF within the `SELECT` clause.\n",
    "    - The UDF will invoke the Lambda function to compute the result.\n",
    "  \n",
    "- **Lambda Invocation Frequency**\n",
    "  - **Invocation Examples**\n",
    "    - Example 1: Lambda is invoked for each row if the parameters are dynamic (e.g., columns in the table).\n",
    "    - Example 2: Lambda is invoked only once if parameters are fixed values.\n",
    "  \n",
    "  - **Invocation in SELECT Clause**\n",
    "    - If used in the `SELECT` clause, Lambda function processes multiple rows in one call, looping through arguments.\n",
    "    - Ensure that Lambda execution does not exceed time limits when handling multiple rows.\n",
    "\n",
    "- **Considerations**\n",
    "  - Be cautious of Lambda's concurrency limits and costs.\n",
    "  - Use Lambda functions efficiently to avoid unnecessary invocations.\n",
    "  - Test queries to optimize Lambda usage and minimize invocation count.\n",
    "\n",
    "- **Practical Example: Creating a UDF with Lambda**\n",
    "  - Create a Redshift cluster and associate it with an IAM role.\n",
    "  - Create tables and insert data for query testing.\n",
    "  - Create a Lambda function in Python 3.8 to process quantity and price.\n",
    "  - Define a UDF in Redshift that uses this Lambda function.\n",
    "  - Query the data using the UDF, passing parameters like `quantity_order` and `price_each`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials - Continuous S3 data ingestion to Amazon Redshift (Copy Job)](https://www.youtube.com/watch?v=2reIpdRYscM&list=PLO95rE9ahzRuUGYApNciILstNNJlvuc6g&index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Tutorials: Amazon Redshift Federated Query with RDS PostgreSQL](https://www.youtube.com/watch?v=vJXZwkch2WY&list=PLO95rE9ahzRuUGYApNciILstNNJlvuc6g&index=3)\n",
    "-   [lab](https://aws-dojo.com/ws37/labs/#google_vignette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
