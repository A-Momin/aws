{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import os, time, json, io, zipfile\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import iam, s3, lf, rds, vpc, ec2, redshift\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "# boto3.setup_default_session(profile_name=\"AMominNJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "redshift_client      = boto3.client('redshift')\n",
    "ec2_client = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource = boto3.resource('ec2', region_name=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [What is Amazon Redshift | How to configure and connect to Redshift](https://www.youtube.com/watch?v=tSHCf1koYk8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes: Configuring and Starting AWS Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **What is Amazon Redshift?**\n",
    "  - A fully managed data warehousing service in the cloud.\n",
    "  - Redshift automatically scales based on the compute demand.\n",
    "  - Supports columnar storage and massive parallel processing for high-performance query execution.\n",
    "  - AWS Redshift Serverless allows automatic scaling and cost-efficient operation without manual infrastructure management.\n",
    "\n",
    "- **Key Advantages of Redshift:**\n",
    "  - Speed and performance in executing queries.\n",
    "  - Automatic backups and scaling.\n",
    "  - Columnar storage for efficient data access.\n",
    "  - Supports materialized views for improving query performance by pre-computing results.\n",
    "  - Offers Massive Parallel Processing (MPP) for distributed query execution across multiple nodes.\n",
    "  - Comparisons with other data warehouse solutions:\n",
    "    - Google BigQuery, Snowflake, Microsoft Azure Synapse Analytics.\n",
    "  - Compression mechanisms and storage on S3.\n",
    "  \n",
    "- **Steps to Create and Configure Redshift Cluster:**\n",
    "  - **1. Create Cluster Subnet Group:**\n",
    "    - Navigate to **Clusters** and create a new cluster.\n",
    "    - Create a **Subnet Group** to choose the VPC and subnets for the Redshift cluster.\n",
    "    - Choose your **VPC** (e.g., default VPC) and select the desired subnets (e.g., `ap-south-1a`, `ap-south-1b`).\n",
    "    - Create the subnet group.\n",
    "\n",
    "  - **2. Create IAM Role for S3 Access (Optional):**\n",
    "    - Navigate to **IAM** and create a role for Redshift.\n",
    "    - Attach the policy `AmazonRedshiftAllCommandsFullAccess`.\n",
    "    - If needed, add additional policies like `S3ReadOnlyAccess` to allow S3 bucket access.\n",
    "    - Attach the role to the Redshift cluster.\n",
    "\n",
    "  - **3. Configure Cluster Settings:**\n",
    "    - Navigate back to the Redshift **Clusters** dashboard.\n",
    "    - Enter a **Cluster Identifier** (e.g., `myydredshift`).\n",
    "    - Choose the **Node Type** (e.g., `dc2.large`) and set the number of nodes.\n",
    "      - **Leader Node** coordinates query execution.\n",
    "      - **Compute Nodes** store the actual data.\n",
    "    - Set **Database Configuration** (username, password, etc.).\n",
    "\n",
    "  - **4. Set Security and Networking:**\n",
    "    - Configure a **Security Group** for Redshift and open necessary ports (e.g., port `5439` for Redshift).\n",
    "    - Assign the cluster to a **VPC** and subnet.\n",
    "    - Optionally enable **Enhanced VPC Routing** for internal traffic within the VPC.\n",
    "\n",
    "  - **5. Enable Backups and Maintenance:**\n",
    "    - Set up **automated backups** and define snapshot retention policies (e.g., retain backups for 7 days).\n",
    "    - Optionally configure **high availability** and replication to other regions.\n",
    "\n",
    "  - **6. Finalizing Cluster Setup:**\n",
    "    - Click **Create Cluster** to launch the Redshift cluster.\n",
    "    - Wait for the cluster creation process to complete (this may take a few minutes).\n",
    "\n",
    "- **Connecting to Redshift:**\n",
    "  - Use the **Query Editor V2** or other SQL tools like **MySQL Workbench** to connect to the cluster.\n",
    "  - Authenticate using database credentials (e.g., `username: redshift_admin`).\n",
    "  - Load sample data or import data from S3 or a local file.\n",
    "  - Execute queries and analyze data in the Redshift Query Editor.\n",
    "\n",
    "- **Additional Redshift Operations:**\n",
    "  - Resize the cluster by adjusting node configurations via the **Actions** menu.\n",
    "  - View and manage existing databases, schemas, and tables from the Query Editor interface.\n",
    "  - Monitor cluster performance and query history.\n",
    "\n",
    "Let me know if you need further refinement on any step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deployment of Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name, datalake_folder_name = 'httx-datalake-bkt', \"S3-Datalake\"\n",
    "catalog_db_name = 'httx-catalog-db'\n",
    "glue_role_name = \"httx-glue-role\" \n",
    "s3_crawler_name = \"httx-s3crawler\"\n",
    "rds_crawler_name = \"httx-rdscrawler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_identifier = \"httx-redshift-cluster-1\"\n",
    "admin_db_username = os.environ['USERNAME']\n",
    "admin_db_password = os.environ['PASSWORD']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [What is Amazon Redshift Serverless | How to configure and connect to Redshift Serverless](https://www.youtube.com/watch?v=mEEbqIdQf7w&list=PLneBjIzDLECkmd-0-gcegDghBdC0hRukz&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [How to load data from S3 to Redshift](https://www.youtube.com/watch?v=AcVNDbSy9L8&list=PLneBjIzDLECkmd-0-gcegDghBdC0hRukz&index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes: load data from S3 to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Setting up the environment**\n",
    "  - Create an S3 bucket\n",
    "    - Name the bucket (e.g., `redshift-demo`)\n",
    "    - Enable necessary options (e.g., block public access, versioning)\n",
    "    - Upload the data file (e.g., `customer.csv`) to the S3 bucket\n",
    "  - Prepare the data\n",
    "    - The dataset includes columns like `index`, `ID`, `first name`, `last name`, `company`, `city`, `country`, `phone numbers`, and `email`\n",
    "    - Ensure the data is in a supported format (e.g., CSV)\n",
    "\n",
    "- **Setting up IAM role**\n",
    "  - Create an IAM role for Redshift\n",
    "    - Use case: Redshift\n",
    "    - Attach appropriate policies (e.g., S3 read-only access)\n",
    "    - Optionally, limit access to specific S3 buckets\n",
    "  - Attach the IAM role to Redshift (either Serverless or Cluster)\n",
    "\n",
    "- **Connecting to Redshift**\n",
    "  - Use the Redshift Query Editor v2\n",
    "  - Create a connection\n",
    "    - Provide credentials (e.g., username: `RsAdmin`, password: set during namespace setup)\n",
    "  - Select the database (e.g., `dev`) and schema (e.g., `public`)\n",
    "\n",
    "- **Loading data into Redshift**\n",
    "  - Choose to load data from the S3 bucket\n",
    "    - Select the S3 bucket and file (e.g., `customer.csv`)\n",
    "    - Specify file format (e.g., CSV) and delimiter\n",
    "    - Configure additional options (e.g., headers, conversion parameters)\n",
    "  - Create or choose the table\n",
    "    - Create a new table (e.g., `customers`) if it doesn't exist\n",
    "    - Map columns (e.g., `index` as primary key)\n",
    "  - Run the `COPY` command\n",
    "    - Specify table, schema, and data format (e.g., CSV)\n",
    "    - Use Redshift's `COPY` command to load data from the S3 bucket to the Redshift table\n",
    "\n",
    "- **Verifying the data load**\n",
    "  - Query the loaded data\n",
    "    - Verify that the data has been loaded into the table\n",
    "    - Run queries to inspect and filter data (e.g., `SELECT * FROM customers WHERE country = 'USA'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
