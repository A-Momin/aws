{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import os, time, json, io, zipfile\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "from emr import create_emr_cluster\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "# boto3.setup_default_session(profile_name=\"AMominNJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ['AWS_DEFAULT_REGION']\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "print(SUBNET_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client           = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource         = boto3.resource('ec2', region_name=REGION)\n",
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "stepfunctions_client = boto3.client('stepfunctions')\n",
    "apigateway_client    = boto3.client('apigateway')\n",
    "lsn_client           = boto3.client('lambda')\n",
    "events_client        = boto3.client('events')\n",
    "sqs_client           = boto3.client('sqs')\n",
    "\n",
    "emr_client = boto3.client('emr', region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [Boto3 EMR Tutorial](https://hands-on.cloud/boto3/emr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [EMR: Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **SecurityConfiguration**\n",
    "\n",
    "    A **Security Configuration** in AWS EMR defines how data is encrypted and secured at rest and in transit. It allows you to enable various security settings for the cluster.\n",
    "\n",
    "    ```python\n",
    "    import boto3\n",
    "\n",
    "    emr_client = boto3.client('emr', region_name='us-east-1')\n",
    "\n",
    "    # Create a security configuration\n",
    "    response = emr_client.create_security_configuration(\n",
    "        Name='MySecurityConfig',\n",
    "        SecurityConfiguration='''{\n",
    "            \"EncryptionConfiguration\": {\n",
    "                \"EnableInTransitEncryption\": true,\n",
    "                \"EnableAtRestEncryption\": true,\n",
    "                \"AtRestEncryptionConfiguration\": {\n",
    "                    \"S3EncryptionConfiguration\": {\n",
    "                        \"EncryptionMode\": \"SSE-S3\"\n",
    "                    }\n",
    "                },\n",
    "                \"InTransitEncryptionConfiguration\": {\n",
    "                    \"TLSCertificateConfiguration\": {\n",
    "                        \"CertificateProviderType\": \"PEM\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }'''\n",
    "    )\n",
    "    print(response)\n",
    "    ```\n",
    "\n",
    "-   **StepConcurrencyLevel**\n",
    "\n",
    "    **Step Concurrency Level** determines how many steps (tasks) can run concurrently in an EMR cluster. By default, EMR executes steps sequentially, but you can adjust this to improve parallelism.\n",
    "\n",
    "    ```python\n",
    "    response = emr_client.run_job_flow(\n",
    "        Name='MyCluster',\n",
    "        ReleaseLabel='emr-6.10.0',\n",
    "        Instances={\n",
    "            'InstanceGroups': [...],\n",
    "            'KeepJobFlowAliveWhenNoSteps': True\n",
    "        },\n",
    "        Steps=[\n",
    "            {'Name': 'Step1', 'HadoopJarStep': {...}, 'ActionOnFailure': 'CONTINUE'},\n",
    "            {'Name': 'Step2', 'HadoopJarStep': {...}, 'ActionOnFailure': 'CONTINUE'}\n",
    "        ],\n",
    "        StepConcurrencyLevel=2  # Allows two steps to run concurrently\n",
    "    )\n",
    "    ```\n",
    "\n",
    "-   **ManagedScalingPolicy**\n",
    "\n",
    "    A **Managed Scaling Policy** in EMR enables automatic adjustment of the cluster's compute resources (instances) based on workload demands. It uses predefined scaling rules.\n",
    "\n",
    "    ```python\n",
    "    response = emr_client.put_managed_scaling_policy(\n",
    "        ClusterId='j-XXXXXXXXXXXXX',\n",
    "        ManagedScalingPolicy={\n",
    "            'ComputeLimits': {\n",
    "                'UnitType': 'InstanceFleetUnits',\n",
    "                'MinimumCapacityUnits': 2,\n",
    "                'MaximumCapacityUnits': 10,\n",
    "                'MaximumOnDemandCapacityUnits': 4,\n",
    "                'MaximumCoreCapacityUnits': 6\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(response)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance Profile and Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instance Profile Creation using CLI [`SUCCESS`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws emr create-default-roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This command creates:\n",
    "\n",
    "    -   `EMR_DefaultRole` for the EMR service.\n",
    "    -   `EMR_EC2_DefaultRole` for EC2 instances.\n",
    "    -   `EMR_AutoScaling_DefaultRole`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Path': '/', 'InstanceProfileName': 'EMR_EC2_DefaultRole', 'InstanceProfileId': 'AIPAVRUVV3SNT2Y5IQA4T', 'Arn': 'arn:aws:iam::381492255899:instance-profile/EMR_EC2_DefaultRole', 'CreateDate': datetime.datetime(2024, 12, 7, 2, 47, 19, tzinfo=tzutc()), 'Roles': [{'Path': '/', 'RoleName': 'EMR_EC2_DefaultRole', 'RoleId': 'AROAVRUVV3SN7L6L3U7GE', 'Arn': 'arn:aws:iam::381492255899:role/EMR_EC2_DefaultRole', 'CreateDate': datetime.datetime(2024, 12, 7, 2, 47, 17, tzinfo=tzutc()), 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}]}]\n"
     ]
    }
   ],
   "source": [
    "response = iam_client.list_instance_profiles()\n",
    "\n",
    "# Extract and return the instance profiles\n",
    "instance_profiles = response[\"InstanceProfiles\"]\n",
    "print(instance_profiles)\n",
    "# print(response[\"InstanceProfiles\"][0]['InstanceProfileName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMR_BUCKET_NAME = 'emr-bkt-' + date.today().strftime('%Y%m%d')  # The name must be unique across all of Amazon S3\n",
    "inputs, outputs, scripts, logs = [\"inputs\", \"outputs\", \"scripts\", \"logs\"]    # List of folders to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create EMR Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME=f\"emr-cluster-{date.today().strftime('%Y%m%d')}\"\n",
    "LogUri, ReleaseLabel, Ec2SubnetId = \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = create_emr_cluster(CLUSTER_NAME, LogUri, ReleaseLabel, Ec2SubnetId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = emr_client.list_clusters(ClusterStates=[\"STARTING\", \"BOOTSTRAPPING\", \"RUNNING\", \"WAITING\"])\n",
    "\n",
    "# Extract ClusterIds\n",
    "clusters = [f\"\"\"\"ClusterId\": {cluster[\"Id\"]}; \"Name\": {cluster[\"Name\"]}; \"Status\": {cluster[\"Status\"][\"State\"]}\"\"\" for cluster in response[\"Clusters\"]]\n",
    "    \n",
    "\n",
    "[print(cluster_info) for cluster_info in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emr_client.describe_cluster(ClusterId=cluster['ClusterId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emr_client.list_clusters(ClusterStates=[\"STARTING\", \"BOOTSTRAPPING\", \"RUNNING\", \"WAITING\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Intro to Amazon EMR - Big Data Tutorial using Spark](https://www.youtube.com/watch?v=8bOgOvz6Tcg&t=667s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "def transform_data(data_source, output_uri):\n",
    "    with SparkSession.builder.appName('test_emr').getOrCreate() as spark:\n",
    "        df = spark.read.option('header', 'true').csv(data_source)\n",
    "        df = df.select(\n",
    "            fn.col(\"Name\").alias(\"name\"),\n",
    "            fn.col(\"Violation Type\").alias(\"violation_type\")\n",
    "        )\n",
    "        df.createOrReplaceTempView(\"restaurant_violations\")\n",
    "        GROUP_BY_QUERY = \"\"\"SELECT name, count(*) as total_red_violations FROM restaurant_violations WHERE violation_type == \"RED\" GROUP BY name\"\"\"\n",
    "        transformed_df = spark.sql(GROUP_BY_QUERY)\n",
    "        print(f\"Number of Rows in SQL Query {transformed_df.count()}\")\n",
    "        transformed_df.write.mode(\"overwrite\").parquet(output_uri)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_source\")\n",
    "    parser.add_argument(\"--output_uri\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    transform_data(args.data_source, args.output_uri)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl = 'private'                             # Set the ACL (e.g., 'private', 'public-read')\n",
    "enable_versioning = False                   # Enable versioning\n",
    "enable_encryption = False                   # Enable server-side encryption\n",
    "\n",
    "\n",
    "s3.create_s3_bucket(EMR_BUCKET_NAME, [inputs, outputs, scripts, logs])\n",
    "\n",
    "\n",
    "local_file1 = os.environ['DATA'] + '/restaurant_violations.csv'  # The local file you want to upload\n",
    "object_name1 = f\"{inputs}/restaurant_violations.csv\"  # The name to save the file as in the S3 bucket\n",
    "\n",
    "# Upload the file\n",
    "s3.upload_file_to_s3(EMR_BUCKET_NAME, local_file1, object_name1)\n",
    "# delete_file_from_s3(EMR_BUCKET_NAME, object_name1)\n",
    "\n",
    "local_file2 = \"../aws_emr/emr_basic/main.py\"\n",
    "pyscript_object = f\"{scripts}/main.py\"  # The name to save the file as in the S3 bucket\n",
    "\n",
    "# Upload the file\n",
    "s3.upload_file_to_s3(EMR_BUCKET_NAME, local_file2, pyscript_object)\n",
    "# delete_file_from_s3(bucket_name, pyscript_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkt_list_res = s3_client.list_objects_v2(Bucket=EMR_BUCKET_NAME)\n",
    "for item in bkt_list_res['Contents']:\n",
    "    print(item['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance Profile and Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instance Profile Creation using CLI [`SUCCESS`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws emr create-default-roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This command creates:\n",
    "\n",
    "    -   `EMR_DefaultRole` for the EMR service.\n",
    "    -   `EMR_EC2_DefaultRole` for EC2 instances.\n",
    "    -   `EMR_AutoScaling_DefaultRole`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Path': '/', 'InstanceProfileName': 'EMR_EC2_DefaultRole', 'InstanceProfileId': 'AIPAVRUVV3SNT2Y5IQA4T', 'Arn': 'arn:aws:iam::381492255899:instance-profile/EMR_EC2_DefaultRole', 'CreateDate': datetime.datetime(2024, 12, 7, 2, 47, 19, tzinfo=tzutc()), 'Roles': [{'Path': '/', 'RoleName': 'EMR_EC2_DefaultRole', 'RoleId': 'AROAVRUVV3SN7L6L3U7GE', 'Arn': 'arn:aws:iam::381492255899:role/EMR_EC2_DefaultRole', 'CreateDate': datetime.datetime(2024, 12, 7, 2, 47, 17, tzinfo=tzutc()), 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}]}]\n"
     ]
    }
   ],
   "source": [
    "response = iam_client.list_instance_profiles()\n",
    "\n",
    "# Extract and return the instance profiles\n",
    "instance_profiles = response[\"InstanceProfiles\"]\n",
    "print(instance_profiles)\n",
    "# print(response[\"InstanceProfiles\"][0]['InstanceProfileName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create EMR Cluster [`SUCCESS`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME=f\"emr-cluster-{date.today().strftime('%Y%m%d')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = emr_client.run_job_flow(\n",
    "    Name=CLUSTER_NAME,  # Name of the EMR cluster\n",
    "    LogUri=f\"s3://{EMR_BUCKET_NAME}/{logs}/\",\n",
    "    ReleaseLabel=\"emr-5.32.0\",\n",
    "    Instances={\n",
    "        \"InstanceGroups\": [\n",
    "            {\n",
    "                \"InstanceRole\": \"MASTER\",\n",
    "                \"InstanceType\": \"m5.xlarge\",\n",
    "                \"InstanceCount\": 1,\n",
    "                \"Market\": \"ON_DEMAND\"\n",
    "            },\n",
    "            {\n",
    "                \"InstanceRole\": \"CORE\",\n",
    "                \"InstanceType\": \"m5.xlarge\",\n",
    "                \"InstanceCount\": 2,\n",
    "                \"Market\": \"ON_DEMAND\"\n",
    "            },\n",
    "            # {\n",
    "            #     \"InstanceRole\": \"TASK\",\n",
    "            #     \"InstanceType\": \"m5.xlarge\",\n",
    "            #     \"InstanceCount\": 1,  # Optional: Add if task nodes are required\n",
    "            #     \"Market\": \"ON_DEMAND\"\n",
    "            # }\n",
    "        ],\n",
    "        \"Ec2KeyName\": \"AMominNJ\",  # EC2 key pair for SSH access\n",
    "        \"KeepJobFlowAliveWhenNoSteps\": True,\n",
    "        \"TerminationProtected\": False,\n",
    "        \"Ec2SubnetId\": SUBNET_ID,  # Replace with your subnet ID\n",
    "        # \"HadoopVersion\": \"2.10.1\",  # Optional: Hadoop version\n",
    "    },\n",
    "    Applications=[\n",
    "        {\"Name\": \"Hadoop\"},\n",
    "        {\"Name\": \"Spark\"},\n",
    "        {\"Name\": \"Hive\"},\n",
    "        {\"Name\": \"Hue\"},\n",
    "        {\"Name\": \"JupyterHub\"},\n",
    "    ],\n",
    "    VisibleToAllUsers=True,\n",
    "    ServiceRole=\"EMR_DefaultRole\",  # IAM role for EMR service\n",
    "    JobFlowRole=\"EMR_EC2_DefaultRole\",  # IAM role for EMR EC2 instances\n",
    "    AutoScalingRole=\"EMR_AutoScaling_DefaultRole\",  # Optional: For auto-scaling\n",
    "    Tags=[\n",
    "        {\"Key\": \"Environment\", \"Value\": \"Development\"},\n",
    "        {\"Key\": \"Project\", \"Value\": \"EMR_DataEngineering\"}\n",
    "    ],\n",
    "    ScaleDownBehavior=\"TERMINATE_AT_TASK_COMPLETION\",  # Scale-down behavior\n",
    "    StepConcurrencyLevel=1,  # Optional: Max concurrent steps\n",
    ")['JobFlowId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = emr_client.list_clusters(ClusterStates=[\"STARTING\", \"BOOTSTRAPPING\", \"RUNNING\", \"WAITING\"])\n",
    "\n",
    "# Extract ClusterIds\n",
    "clusters = [f\"\"\"\"ClusterId\": {cluster[\"Id\"]}; \"Name\": {cluster[\"Name\"]}; \"Status\": {cluster[\"Status\"][\"State\"]}\"\"\" for cluster in response[\"Clusters\"]]\n",
    "    \n",
    "\n",
    "[print(cluster_info) for cluster_info in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emr_client.describe_cluster(ClusterId=cluster['ClusterId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emr_client.list_clusters(ClusterStates=[\"STARTING\", \"BOOTSTRAPPING\", \"RUNNING\", \"WAITING\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = f\"s3://{EMR_BUCKET_NAME}/{object_name1}\"\n",
    "output_uri = f\"s3://{EMR_BUCKET_NAME}/{outputs}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step definition\n",
    "pyspark_step = {\n",
    "    'Name': 'TestRun2',\n",
    "    'ActionOnFailure': 'CONTINUE',\n",
    "    'HadoopJarStep': {\n",
    "        'Jar': 'command-runner.jar',\n",
    "        'Args': [\n",
    "            'spark-submit',\n",
    "            f's3://{EMR_BUCKET_NAME}/{pyscript_object}',\n",
    "            '--data_source', data_source,\n",
    "            '--output_uri', output_uri\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add step to the cluster\n",
    "response = emr_client.add_job_flow_steps(\n",
    "    JobFlowId=cluster_id,\n",
    "    Steps=[pyspark_step]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the response\n",
    "step_id = response['StepIds'][0]\n",
    "print(f\"Step added with ID: {step_id}\")\n",
    "\n",
    "# Monitor the step\n",
    "step_status = emr_client.describe_step(ClusterId=cluster_id, StepId=step_id)\n",
    "print(\"Step Status:\", step_status['Step']['Status']['State'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alternative Option: CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = ['spark-submit',f's3://{EMR_BUCKET_NAME}/{pyscript_object}','--data_source', data_source,'--output_uri', output_uri]\n",
    "STEPS = f\"\"\"Type=CUSTOM_JAR,Name=MyStepName,ActionOnFailure=CONTINUE,Jar=command-runner.jar,Args=\"{part}\\\"\"\"\"\n",
    "print(part)\n",
    "print(STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws emr add-steps --cluster-id {cluster_id} --region {REGION} --steps {STEPS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Another Method of using CLI** [`NOT TESTED`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps.json\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"Name\": \"Step 1\",\n",
    "    \"ActionOnFailure\": \"CONTINUE\",\n",
    "    \"HadoopJarStep\": {\n",
    "      \"Jar\": \"command-runner.jar\",\n",
    "      \"Args\": [\"spark-submit\", \"s3://emr-bkt-20241206/scripts/main.py\", \"--data_source\", \"s3://emr-bkt-20241206/inputs/restaurant_violations.csv\", \"--output_uri\", \"s3://emr-bkt-20241206/outputs\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"Name\": \"Step 2\",\n",
    "    \"ActionOnFailure\": \"CONTINUE\",\n",
    "    \"HadoopJarStep\": {\n",
    "      \"Jar\": \"command-runner.jar\",\n",
    "      \"Args\": [\"spark-submit\", \"--deploy-mode\", \"cluster\", \"--master\", \"yarn\", \"s3://your-bucket/script2.py\"]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "aws emr add-steps \\\n",
    "    --cluster-id j-XXXXXXXX \\\n",
    "    --steps file://steps.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runn Script from the Master Node [`NOT TESTED YET`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"spark-submit main.py --data_source {data_source} --output_uri {output_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `$ scp ./main.py -i ~/.ssh/AMominNJ.pem hadoop@ec2-3-235-170-221.compute-1.amazonaws.com:/home/hadoop`\n",
    "-   `$ ssh -i ~/.ssh/AMominNJ.pem hadoop@ec2-3-235-170-221.compute-1.amazonaws.com:/home/hadoop`\n",
    "-   `$ spark-submit main.py --data_source {data_source} --output_uri {output_uri}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "bucket1 = s3_resource.Bucket(EMR_BUCKET_NAME)\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "bucket1.objects.all().delete()\n",
    "\n",
    "# Delete all object versions (if versioning is enabled)\n",
    "# bucket1.object_versions.all().delete()\n",
    "\n",
    "# Finally, delete the bucket\n",
    "bucket1.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr_client.terminate_job_flows(JobFlowIds=[cluster_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = iam_client.delete_instance_profile(InstanceProfileName='emr_instance_profile')\n",
    "# print(\"Instance profile deleted:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE IAM ROLE AT THE END AFTER DELETING ALL OTHER RESOURCES.\n",
    "iam.delete_iam_role(\"EMR_DefaultRole\")\n",
    "iam.delete_iam_role(\"EMR_EC2_DefaultRole\")\n",
    "iam.delete_iam_role(\"EMR_AutoScaling_DefaultRole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   [Automating EMR Serverless Workload |Creating|Submitting | Destroying EMR Cluster using Step Function](https://www.youtube.com/watch?v=V7bFwXBN5xc&t=199s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [code](https://github.com/soumilshah1995/Automating-EMR-Serverless-Workload-Creating-Submitting-Destroying-EMR-Cluster-using-Step-Funct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"Comment\": \"A description of my state machine\",\n",
    "  \"StartAt\": \"Create New EMR Application\",\n",
    "  \"States\": {\n",
    "    \"Create New EMR Application\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"ResultPath\": \"$.CreateEMRCluster\",\n",
    "      \"Next\": \"Start EMR Serverless Application\",\n",
    "      \"Parameters\": {\n",
    "        \"Architecture\": \"X86_64\",\n",
    "        \"ClientToken.$\":\"States.UUID()\",\n",
    "        \"ReleaseLabel.$\": \"$.emr_cluster.ReleaseLabel\",\n",
    "        \"Type.$\": \"$.emr_cluster.Type\",\n",
    "        \"Name\": \"datateam\",\n",
    "        \"NetworkConfiguration\": {\n",
    "          \"SecurityGroupIds\": [ \"sg-0f82bcb99a2878231\" ],\n",
    "          \"SubnetIds\": [ \"subnet-05551ec8e1006b370\",\"subnet-03576afd62b50a982\" ]\n",
    "        }\n",
    "      },\n",
    "      \"Resource\": \"arn:aws:states:::aws-sdk:emrserverless:createApplication\"\n",
    "    },\n",
    "    \"Start EMR Serverless Application\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"ResultPath\": \"$.StartEMRApplication\",\n",
    "      \"Next\": \"Start EMR Job wait for CallBack\",\n",
    "      \"Parameters\": {\n",
    "        \"ApplicationId.$\": \"$.CreateEMRCluster.ApplicationId\"\n",
    "      },\n",
    "      \"Resource\": \"arn:aws:states:::aws-sdk:emrserverless:startApplication\"\n",
    "    },\n",
    "    \"Start EMR Job wait for CallBack\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"ResultPath\": \"$.WaitForCallBack\",\n",
    "      \"Catch\":[\n",
    "        {\n",
    "          \"ErrorEquals\":[\n",
    "            \"States.TaskFailed\"\n",
    "          ],\n",
    "          \"Next\":\"wait_2_minutes\"\n",
    "        },\n",
    "        {\n",
    "          \"ErrorEquals\":[\n",
    "            \"States.ALL\"\n",
    "          ],\n",
    "          \"Next\":\"wait_2_minutes\"\n",
    "        }\n",
    "      ],\n",
    "      \"Parameters\": {\n",
    "        \"ApplicationId.$\": \"$.CreateEMRCluster.ApplicationId\",\n",
    "        \"ClientToken.$\": \"States.UUID()\",\n",
    "        \"ExecutionRoleArn.$\": \"$.ExecutionArn\",\n",
    "        \"JobDriver\": {\n",
    "          \"SparkSubmit\": {\n",
    "            \"EntryPoint.$\": \"$.ScriptPath\",\n",
    "            \"EntryPointArguments.$\":  \"States.Array($$.Task.Token)\",\n",
    "            \"SparkSubmitParameters.$\": \"$.SparkSubmitParameters\"\n",
    "          }\n",
    "        },\n",
    "        \"Name.$\": \"$.JobName\"\n",
    "      },\n",
    "      \"Resource\": \"arn:aws:states:::aws-sdk:emrserverless:startJobRun.waitForTaskToken\",\n",
    "      \"Next\": \"wait_2_minutes\"\n",
    "    },\n",
    "    \"wait_2_minutes\": {\n",
    "      \"Type\": \"Wait\",\n",
    "      \"Seconds\": 140,\n",
    "      \"Next\": \"Stop EMR Serverless Application\"\n",
    "    },\n",
    "    \"Stop EMR Serverless Application\": {\n",
    "      \"ResultPath\": \"$.StopApplication\",\n",
    "      \"Type\": \"Task\",\n",
    "      \"Next\": \"Wait for Application to Stop\",\n",
    "      \"Resource\": \"arn:aws:states:::aws-sdk:emrserverless:stopApplication\",\n",
    "      \"Parameters\": {\n",
    "        \"ApplicationId.$\":  \"$.CreateEMRCluster.ApplicationId\"\n",
    "      }\n",
    "    },\n",
    "    \"Wait for Application to Stop\": {\n",
    "      \"Type\": \"Wait\",\n",
    "      \"Seconds\": 140,\n",
    "      \"Next\": \"Delete EMR Serverless Application\"\n",
    "    },\n",
    "    \"Delete EMR Serverless Application\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"ResultPath\": \"$.DeleteEMRJob\",\n",
    "      \"End\": true,\n",
    "      \"Parameters\": {\n",
    "        \"ApplicationId.$\": \"$.CreateEMRCluster.ApplicationId\"\n",
    "      },\n",
    "      \"Resource\": \"arn:aws:states:::aws-sdk:emrserverless:deleteApplication\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS EMR by AWS Tutorials](https://www.youtube.com/playlist?list=PLO95rE9ahzRt42F77Gikc0MNZbv8z7F6N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **IAM Roles**: Ensure that the roles `EMR_EC2_DefaultRole` and `EMR_DefaultRole` exist and have necessary permissions.\n",
    "2. **S3 Buckets**: Replace `s3://my-emr-logs-bucket/` and `s3://my-bootstrap-scripts/bootstrap.sh` with your S3 paths.\n",
    "3. **Networking**: Update the `Ec2SubnetId` to match your network configuration.\n",
    "4. **Customizations**: Modify or extend the `Configurations`, `Steps`, or `BootstrapActions` as needed.\n",
    "5. **Security**: Avoid hardcoding sensitive information like passwords. Use AWS Secrets Manager or environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [lab: Absolute Beginners Tutorial for Amazon EMR](https://aws-dojo.com/ws34/labs/)\n",
    "-   [lab: Using Amazon EMR with AWS Glue Catalog](https://aws-dojo.com/ws41/labs/#google_vignette)\n",
    "-   [lab: Using Transient Amazon EMR Cluster](https://aws-dojo.com/excercises/excercise45/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMR_BKT = \"httx-emr-bkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl = 'public-read'                         # Set the ACL (e.g., 'private', 'public-read')\n",
    "enable_versioning = False                   # Enable versioning\n",
    "enable_encryption = False                   # Enable server-side encryption\n",
    "\n",
    "folders = [\"inputs\", \"outputs\", \"scripts\", \"logs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.create_bucket(Bucket=EMR_BKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Put Object (Upload)**: Uploads an object directly to S3 (binary or text content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client.put_object(Bucket='my-bucket', Key='new_file.txt', Body=b'Hello, World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s3_client.put_object(Bucket=EMR_BKT, Key=folder) for folder in folders]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **List All Buckets**: Lists all the buckets in your S3 account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_buckets()\n",
    "print(response)\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'Bucket: {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Upload a File to S3**: Uploads a file to a specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file('./customers.csv', EMR_BKT, 'source/customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket1 = s3.Bucket(EMR_BKT)\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "bucket1.objects.all().delete()\n",
    "\n",
    "# Delete all object versions (if versioning is enabled)\n",
    "# bucket1.object_versions.all().delete()\n",
    "\n",
    "# Finally, delete the bucket\n",
    "bucket1.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
